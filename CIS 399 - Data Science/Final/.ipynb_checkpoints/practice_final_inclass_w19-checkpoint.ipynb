{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh4uU7eob-bc"
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Practice Final Exam\n",
    "</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXW0jSlfipja"
   },
   "source": [
    "<h2>Question 1: import `ann_flex` and others (15 points)</h2>\n",
    "\n",
    "This is from your take home portion. If you successfully bring in the flex functions, you get the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "G8rLH27bbevq",
    "outputId": "97993c5b-aaf9-4dfa-cea2-1d7fcc6d30f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_flex\t ann_flex_predictor\t ann_flex_tester\t ann_predictor\t ann_simple\t ann_simple_batch\t ann_tester\t from_scratch\t from_scratch_batch\t \n",
      "from_scratch_flex\t mse\t mse_der\t sigmoid\t sigmoid_der\t \n"
     ]
    }
   ],
   "source": [
    "from library_w19_deep_2 import *\n",
    "\n",
    "%who function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn2536_DVRFX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IP18ATeIua5"
   },
   "outputs": [],
   "source": [
    "with open('/Users/Noah/Documents/My Documents/2018:2019/2. Winter/CIS 399/Final/loan_table_week4.csv', 'r') as f:\n",
    "  loan_table = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "2e-PADzZhEO0",
    "outputId": "f3b16763-054f-49a2-9707-f7b3d29735c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
       "       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
       "       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status',\n",
       "       'no_lam', 'filled_lam', 'pa_Rural', 'pa_Semiurban', 'pa_Urban',\n",
       "       'pa_nan', 'lam_bin', 'lam_Low', 'lam_Average', 'lam_High', 'ch_bad',\n",
       "       'ch_good', 'ch_nan', 'apin_bin', 'apin_low', 'apin_average',\n",
       "       'apin_high', 'apin_nan', 'dep_0', 'dep_1', 'dep_2', 'dep_3+',\n",
       "       'dep_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "an3i3EhpkywZ"
   },
   "source": [
    "<h2>Question 2: predict a new column in `loan_table`</h2>\n",
    "<p>\n",
    "I am going to give you the goal and let you fill in the steps. The goal is to use `ann_flex` to train a model for predicting a new column in `loan_table`. Here are specifics:\n",
    "  \n",
    "  1.  Drop all rows with a `NaN` in any column. I was left with 480 rows when I did this.\n",
    "  1. Create a new column `coap_binned` that has 2 values, `Low and High`. Use `qcut` to get the 2 bins from `CoapplicantIncome`.\n",
    "  2. For your sample, use these columns: 'lam_normed, 'pa_Rural', 'pa_Semiurban', 'pa_Urban', 'ch_bad', 'ch_good'. You will have create `lam_normed`, which is the normed version of `LoanAmount`. You can see you will need 6 input nodes for your ANN.\n",
    "  4. I would like 2 output nodes, corresponding to the one-hot encoding of `coap_binned`.\n",
    "  \n",
    "Match my intermediate results shown below. And compute the accuracy against the training table and match my accuracy.\n",
    "  <pre>\n",
    "feature_set[0]\n",
    "array([1.        , 0.        , 0.        , 0.        , 1.        , 0.21333333])\n",
    "\n",
    "(weights, biases) = from_scratch_flex(feature_set, labels, hypers={'epochs':3000, 'cost-reporting': 1000})\n",
    "(0, 0.5440918077466073)\n",
    "(1000, 0.5467307425352096)\n",
    "(2000, 0.546736672684983)\n",
    "(3000, 0.546736672684985)\n",
    "\n",
    "predictions = ann_flex_tester(feature_set, weights, biases)\n",
    "predictions[:10]\n",
    "[array([0.66764488, 0.33235512]),\n",
    " array([0.8547766, 0.1452234]),\n",
    " array([0.81404568, 0.18595432]),\n",
    " array([0.79598459, 0.20401541]),\n",
    " array([0.66163692, 0.33836308]),\n",
    " array([0.83390732, 0.16609268]),\n",
    " array([0.77282866, 0.22717134]),\n",
    " array([0.77089266, 0.22910734]),\n",
    " array([0.7522263, 0.2477737]),\n",
    " array([0.85203315, 0.14796685])]\n",
    " \n",
    " accuracy: 0.50625\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ZzWKRykGM5Vk",
    "outputId": "c3bcc180-1824-440b-ab1b-4492f61d2571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.21333333])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop all rows with a NaN in any column. I was left with 480 rows when I did this.\n",
    "loan_table = loan_table.dropna()\n",
    "\n",
    "#Create a new column coap_binned that has 2 values, Low and High. Use qcut to get the 2 bins from CoapplicantIncome.\n",
    "loan_table['coap_binned'] = pd.qcut(loan_table['CoapplicantIncome'], 2, ['Low', 'High'])\n",
    "\n",
    "#columns: 'lam_normed, 'pa_Rural', 'pa_Semiurban', 'pa_Urban', 'ch_bad', 'ch_good'. \n",
    "#You will have create lam_normed, which is the normed version of LoanAmount.\n",
    "loan_table[\"lam_normed\"] = loan_table[\"LoanAmount\"] / loan_table[\"LoanAmount\"].max()\n",
    "ann_table = loan_table[['pa_Rural', 'pa_Semiurban', 'pa_Urban', 'ch_bad', 'ch_good','lam_normed']]\n",
    "feature_set = ann_table.values\n",
    "\n",
    "#I would like 2 output nodes, corresponding to the one-hot encoding of coap_binned.\n",
    "one_hot_coap = pd.get_dummies(loan_table[\"coap_binned\"], prefix = \"coap\", dummy_na = False)\n",
    "loan_table = loan_table.join(one_hot_coap)\n",
    "labels = loan_table[['coap_Low','coap_High']].values\n",
    "\n",
    "#print(labels)\n",
    "print(feature_set.shape)\n",
    "feature_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "GXCxriyz1Azc",
    "outputId": "9e95119c-3601-459a-dc92-3266d1058025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.5440918077466068)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-e57bca7fa68c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_scratch_flex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cost-reporting'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/My Documents/2018:2019/2. Winter/CIS 399/Final/library_w19_deep_2.py\u001b[0m in \u001b[0;36mfrom_scratch_flex\u001b[0;34m(samples, labels, hypers)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mann_flex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mann_flex_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/My Documents/2018:2019/2. Winter/CIS 399/Final/library_w19_deep_2.py\u001b[0m in \u001b[0;36mann_flex\u001b[0;34m(all_samples, all_labels, weights, biases, hypers)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mfsig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mzv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m#compute error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2051\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0motypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2052\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 res = tuple([array(x, copy=False, subok=True, dtype=t)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(weights, biases) = from_scratch_flex(feature_set, labels, hypers={'epochs':3000, 'cost-reporting': 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MF9baNMQ1ikg",
    "outputId": "09cfd359-f2eb-4612-d63b-60ba4fda9a92"
   },
   "outputs": [],
   "source": [
    "ann_flex_predictor(feature_set[0], weights, biases)  #try it on first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "OvqdDxzQ16_k",
    "outputId": "0c02f8bd-c3f6-45c4-cb46-45bfa3d4cc87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.66764488, 0.33235512]),\n",
       " array([0.8547766, 0.1452234]),\n",
       " array([0.81404568, 0.18595432]),\n",
       " array([0.79598459, 0.20401541]),\n",
       " array([0.66163692, 0.33836308]),\n",
       " array([0.83390732, 0.16609268]),\n",
       " array([0.77282866, 0.22717134]),\n",
       " array([0.77089266, 0.22910734]),\n",
       " array([0.7522263, 0.2477737]),\n",
       " array([0.85203315, 0.14796685])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ann_flex_tester(feature_set, weights, biases)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUVqmwh3g4vv"
   },
   "source": [
    "Note: I used `pair[0]>pair[1]` to get `(1,0)` else `(0,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U7-WTDTi3KYT",
    "outputId": "94f0e400-6778-44e8-f14b-1b40efbf6d4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50625"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 0\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i][0] > predictions[i][1]):\n",
    "        if(labels[i][0] == 1):\n",
    "            accuracy += 1  \n",
    "    else:\n",
    "        if(labels[i][0] == 0):\n",
    "            accuracy += 1\n",
    "        \n",
    "        \n",
    "my_accuracy = accuracy/len(predictions)\n",
    "my_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kij1-pe-gXOV"
   },
   "source": [
    "<h2>Question 3: Momentum</h2>\n",
    "\n",
    "One technique for avoiding big weight-change swings is to use the weight-change history to temper the current weight-change. This is called using momentum (jargon). I want you to explore the momentum algorithm without trying to modify `ann_flex`. Once you are happy with your algorithm, you could consider building a new `ann_mo` function that uses your weight change algorithm. But that is not part of the exam.\n",
    "<p>\n",
    "  To be able to test the alogirthm separately, we will need to set up a bit of simulation. We will simulate pieces of `ann_flex`. In particular, we will simulate obtaining a weight change matrix. We will just randomly make one up,over and over, for testing. Once we have such a matrix, our momentum algorithm can kick in.\n",
    "<p>\n",
    "  So what is this momentum algorithm all about? The general idea is to interpose a momentum calculation inbetween computing a weight-change with back-propogation and actually changing the weights. The approach is keep a record of the weight changes we have computed in the past. How for in the past should we go?  We need a new hyper-parameter `momentum_n`. It says how far back in history we should keep a record. Assume this ranges from 0 (no history kept) to 5 (5 past weight changes kept).\n",
    "  <p>\n",
    "To compute the actual change to the weights, we use this formula. You can see as we move further into the past, we give less influence to past changes.\n",
    "    <p>\n",
    "      `weights = weights + (-current_change - (.5*past1 + .4*past2 + .3*past3 + .2*past4 + .1*past5)`.\n",
    "    <p>\n",
    "Here, `current_change` is the weight changes we obtained from back-propogation. Then `past1` is the actual change calculated 1 step ago (i.e., for the sample before this one). I say \"actual\" to mean the raw weight change obtained by back-propogation and before modified by momentum. `Past2` is from 2 steps ago, etc. This assumes `momentum_n` is set to 5. If it is smaller, then we will lop off the right terms in the formula. If it is 0, then no modificaiton is made to current_change.\n",
    "<p>\n",
    "I hope you infer that you will have to keep track of `past1, past2`, etc. I used a list of weight matrices. If `momentum_n` is 5, then I had a list of 5 weight matrices. I chose to treat the first item in the list as oldest. After calculating the weight change using the formula above, I slid everything in the list left and put current_change into the right most slot. You do not have to do it this way! There are likely more elegant ways of doing it.\n",
    "  <p>\n",
    "    I'll give you pieces of the simulation loop below. I print out the values of `weights` and my history list for every new sample. You should be able to match my results. Notice the first thing printed are the weights and my history list prior to starting the simulation loop. Then I print them at the end of each loop iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4956
    },
    "colab_type": "code",
    "id": "403xa6LMeD4J",
    "outputId": "aacafa5f-84cd-47ba-820a-3e78b426b6a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02509198  0.09014286]\n",
      " [ 0.04639879  0.0197317 ]\n",
      " [-0.06879627 -0.0688011 ]\n",
      " [-0.08838328  0.07323523]\n",
      " [ 0.020223    0.04161452]\n",
      " [-0.0958831   0.09398197]]\n",
      "==============================\n",
      "==============================\n",
      "[[ 0.06648853 -0.05753218]\n",
      " [-0.06363501 -0.0633191 ]\n",
      " [-0.03915155  0.00495129]\n",
      " [-0.013611   -0.04175417]\n",
      " [ 0.02237058 -0.07210123]\n",
      " [-0.04157107 -0.02672763]]\n",
      "[[[ 0.04139655  0.03261068]\n",
      "  [-0.01723622 -0.0435874 ]\n",
      "  [-0.10794782 -0.06384981]\n",
      "  [-0.10199427  0.03148106]\n",
      "  [ 0.04259358 -0.03048671]\n",
      "  [-0.13745417  0.06725434]]\n",
      "\n",
      " [[ 0.04139655  0.03261068]\n",
      "  [-0.01723622 -0.0435874 ]\n",
      "  [-0.10794782 -0.06384981]\n",
      "  [-0.10199427  0.03148106]\n",
      "  [ 0.04259358 -0.03048671]\n",
      "  [-0.13745417  0.06725434]]\n",
      "\n",
      " [[ 0.04139655  0.03261068]\n",
      "  [-0.01723622 -0.0435874 ]\n",
      "  [-0.10794782 -0.06384981]\n",
      "  [-0.10199427  0.03148106]\n",
      "  [ 0.04259358 -0.03048671]\n",
      "  [-0.13745417  0.06725434]]\n",
      "\n",
      " [[ 0.04139655  0.03261068]\n",
      "  [-0.01723622 -0.0435874 ]\n",
      "  [-0.10794782 -0.06384981]\n",
      "  [-0.10199427  0.03148106]\n",
      "  [ 0.04259358 -0.03048671]\n",
      "  [-0.13745417  0.06725434]]\n",
      "\n",
      " [[ 0.04139655  0.03261068]\n",
      "  [-0.01723622 -0.0435874 ]\n",
      "  [-0.10794782 -0.06384981]\n",
      "  [-0.10199427  0.03148106]\n",
      "  [ 0.04259358 -0.03048671]\n",
      "  [-0.13745417  0.06725434]]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]]\n",
      "==============================\n",
      "[[-0.008786    0.05703519]\n",
      " [-0.06006524  0.00284689]\n",
      " [ 0.01848291 -0.09070992]\n",
      " [ 0.02150897 -0.06589518]\n",
      " [-0.08698968  0.08977711]\n",
      " [ 0.09312641  0.06167947]]\n",
      "[[[-0.00063372  0.11841196]\n",
      "  [-0.04548396 -0.00908096]\n",
      "  [-0.06988913 -0.15703537]\n",
      "  [-0.07367981 -0.01353703]\n",
      "  [-0.05558139  0.09534101]\n",
      "  [-0.02354223  0.14229762]]\n",
      "\n",
      " [[ 0.03261055  0.08964588]\n",
      "  [-0.07730146 -0.04074051]\n",
      "  [-0.08946491 -0.15455973]\n",
      "  [-0.0804853  -0.03441412]\n",
      "  [-0.0443961   0.0592904 ]\n",
      "  [-0.04432776  0.12893381]]\n",
      "\n",
      " [[ 0.03261055  0.08964588]\n",
      "  [-0.07730146 -0.04074051]\n",
      "  [-0.08946491 -0.15455973]\n",
      "  [-0.0804853  -0.03441412]\n",
      "  [-0.0443961   0.0592904 ]\n",
      "  [-0.04432776  0.12893381]]\n",
      "\n",
      " [[ 0.03261055  0.08964588]\n",
      "  [-0.07730146 -0.04074051]\n",
      "  [-0.08946491 -0.15455973]\n",
      "  [-0.0804853  -0.03441412]\n",
      "  [-0.0443961   0.0592904 ]\n",
      "  [-0.04432776  0.12893381]]\n",
      "\n",
      " [[ 0.03261055  0.08964588]\n",
      "  [-0.07730146 -0.04074051]\n",
      "  [-0.08946491 -0.15455973]\n",
      "  [-0.0804853  -0.03441412]\n",
      "  [-0.0443961   0.0592904 ]\n",
      "  [-0.04432776  0.12893381]]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]]\n",
      "==============================\n",
      "[[-0.03907725 -0.08046558]\n",
      " [ 0.03684661 -0.0119695 ]\n",
      " [-0.07559235 -0.00096462]\n",
      " [-0.0931223   0.08186408]\n",
      " [-0.048244    0.03250446]\n",
      " [-0.03765778  0.0040136 ]]\n",
      "[[[-0.07295523  0.06671248]\n",
      "  [ 0.02318015  0.01060908]\n",
      "  [-0.12590571 -0.16047563]\n",
      "  [-0.1599966   0.08920413]\n",
      "  [-0.11501068  0.16389608]\n",
      "  [-0.04041448  0.15967504]]\n",
      "\n",
      " [[-0.0029523  -0.01363378]\n",
      "  [-0.01642876 -0.05384877]\n",
      "  [-0.17245043 -0.11924038]\n",
      "  [-0.18221119  0.07380803]\n",
      "  [-0.05784423  0.05588401]\n",
      "  [-0.11923611  0.10827563]]\n",
      "\n",
      " [[-0.0064667   0.0091803 ]\n",
      "  [-0.04045486 -0.05271001]\n",
      "  [-0.16505726 -0.15552435]\n",
      "  [-0.1736076   0.04744996]\n",
      "  [-0.0926401   0.09179485]\n",
      "  [-0.08198555  0.13294741]]\n",
      "\n",
      " [[-0.0064667   0.0091803 ]\n",
      "  [-0.04045486 -0.05271001]\n",
      "  [-0.16505726 -0.15552435]\n",
      "  [-0.1736076   0.04744996]\n",
      "  [-0.0926401   0.09179485]\n",
      "  [-0.08198555  0.13294741]]\n",
      "\n",
      " [[-0.0064667   0.0091803 ]\n",
      "  [-0.04045486 -0.05271001]\n",
      "  [-0.16505726 -0.15552435]\n",
      "  [-0.1736076   0.04744996]\n",
      "  [-0.0926401   0.09179485]\n",
      "  [-0.08198555  0.13294741]]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]]\n",
      "==============================\n",
      "[[ 0.00934206 -0.06302911]\n",
      " [ 0.09391693  0.05502656]\n",
      " [ 0.08789979  0.07896547]\n",
      " [ 0.01958     0.08437485]\n",
      " [-0.0823015  -0.06080343]\n",
      " [-0.09095454 -0.03493393]]\n",
      "[[[-9.68574338e-02  3.24494564e-02]\n",
      "  [ 1.48914579e-01  9.72951970e-02]\n",
      "  [-1.84301472e-02 -8.39858044e-02]\n",
      "  [-1.33611109e-01  1.94456067e-01]\n",
      "  [-2.08497472e-01  1.39143266e-01]\n",
      "  [-1.10583486e-01  1.38104926e-01]]\n",
      "\n",
      " [[ 9.90416100e-03 -9.94769642e-02]\n",
      "  [ 1.01514264e-01  3.90397668e-05]\n",
      "  [-9.19438053e-02 -3.99094099e-03]\n",
      "  [-1.71234780e-01  1.84540950e-01]\n",
      "  [-1.05349858e-01 -4.08302615e-02]\n",
      "  [-2.47441217e-01  4.86699034e-02]]\n",
      "\n",
      " [[ 1.45985323e-02 -2.97091372e-02]\n",
      "  [ 4.24080875e-02  5.90740029e-03]\n",
      "  [-5.44797684e-02 -7.62694896e-02]\n",
      "  [-1.26090915e-01  1.07265585e-01]\n",
      "  [-1.60468402e-01  2.12400874e-02]\n",
      "  [-1.61642756e-01  9.68093978e-02]]\n",
      "\n",
      " [[ 2.87535848e-03 -5.38488104e-02]\n",
      "  [ 5.34620691e-02  2.31654991e-03]\n",
      "  [-7.71574743e-02 -7.65588750e-02]\n",
      "  [-1.54027603e-01  1.31824809e-01]\n",
      "  [-1.74941603e-01  3.09914245e-02]\n",
      "  [-1.72940092e-01  9.80134791e-02]]\n",
      "\n",
      " [[ 2.87535848e-03 -5.38488104e-02]\n",
      "  [ 5.34620691e-02  2.31654991e-03]\n",
      "  [-7.71574743e-02 -7.65588750e-02]\n",
      "  [-1.54027603e-01  1.31824809e-01]\n",
      "  [-1.74941603e-01  3.09914245e-02]\n",
      "  [-1.72940092e-01  9.80134791e-02]]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.00934206 -0.06302911]\n",
      "  [ 0.09391693  0.05502656]\n",
      "  [ 0.08789979  0.07896547]\n",
      "  [ 0.01958     0.08437485]\n",
      "  [-0.0823015  -0.06080343]\n",
      "  [-0.09095454 -0.03493393]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]]\n",
      "==============================\n",
      "[[-0.02226454 -0.04573019]\n",
      " [ 0.0657475  -0.02864933]\n",
      " [-0.0438131   0.00853922]\n",
      " [-0.07181516  0.0604394 ]\n",
      " [-0.08508987  0.09737739]\n",
      " [ 0.05444895 -0.06025686]]\n",
      "[[[-0.15236624  0.01548535]\n",
      "  [ 0.24647958  0.10030541]\n",
      "  [-0.04266747 -0.07792223]\n",
      "  [-0.19862077  0.27577255]\n",
      "  [-0.30477263  0.27257127]\n",
      "  [-0.035349    0.09121188]]\n",
      "\n",
      " [[-0.00884598 -0.16802123]\n",
      "  [ 0.19128786 -0.02974905]\n",
      "  [-0.14315007  0.04083224]\n",
      "  [-0.25165352  0.27133842]\n",
      "  [-0.15564386  0.02063628]\n",
      "  [-0.23024283 -0.03625875]]\n",
      "\n",
      " [[ 0.00405716 -0.05129966]\n",
      "  [ 0.09710161 -0.01915108]\n",
      "  [-0.07561516 -0.06744089]\n",
      "  [-0.16996938  0.14314576]\n",
      "  [-0.23108507  0.10886614]\n",
      "  [-0.09589647  0.03534845]]\n",
      "\n",
      " [[-0.02125759 -0.08697318]\n",
      "  [ 0.10042619 -0.0373381 ]\n",
      "  [-0.13855053 -0.08381275]\n",
      "  [-0.22975876  0.17538924]\n",
      "  [-0.24357117  0.1405295 ]\n",
      "  [-0.10030023  0.0447434 ]]\n",
      "\n",
      " [[-0.01938918 -0.099579  ]\n",
      "  [ 0.11920957 -0.02633278]\n",
      "  [-0.12097057 -0.06801966]\n",
      "  [-0.22584276  0.19226421]\n",
      "  [-0.26003147  0.12836881]\n",
      "  [-0.11849114  0.03775662]]]\n",
      "==============================\n",
      "[[[-0.02226454 -0.04573019]\n",
      "  [ 0.0657475  -0.02864933]\n",
      "  [-0.0438131   0.00853922]\n",
      "  [-0.07181516  0.0604394 ]\n",
      "  [-0.08508987  0.09737739]\n",
      "  [ 0.05444895 -0.06025686]]\n",
      "\n",
      " [[ 0.00934206 -0.06302911]\n",
      "  [ 0.09391693  0.05502656]\n",
      "  [ 0.08789979  0.07896547]\n",
      "  [ 0.01958     0.08437485]\n",
      "  [-0.0823015  -0.06080343]\n",
      "  [-0.09095454 -0.03493393]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]]\n",
      "==============================\n",
      "[[-0.09889558  0.06309229]\n",
      " [ 0.04137147  0.04580143]\n",
      " [ 0.05425407 -0.08519107]\n",
      " [-0.02830685 -0.07682619]\n",
      " [ 0.07262069  0.02465963]\n",
      " [-0.0338204  -0.08728833]]\n",
      "[[[-0.28450608  0.10734373]\n",
      "  [ 0.31966856  0.17776639]\n",
      "  [ 0.03116238 -0.16558894]\n",
      "  [-0.22012212  0.21982345]\n",
      "  [-0.24333724  0.33328151]\n",
      "  [-0.04838386  0.01728736]]\n",
      "\n",
      " [[-0.10422716 -0.12774303]\n",
      "  [ 0.25668543  0.01491363]\n",
      "  [-0.09628917 -0.00807486]\n",
      "  [-0.28856397  0.2208703 ]\n",
      "  [-0.0482273   0.00938507]\n",
      "  [-0.30131378 -0.14821887]]\n",
      "\n",
      " [[-0.08311524  0.0359323 ]\n",
      "  [ 0.12741909  0.0302412 ]\n",
      "  [ 0.00131661 -0.15234257]\n",
      "  [-0.17033955  0.04176035]\n",
      "  [-0.14399119  0.12377443]\n",
      "  [-0.11841953 -0.05314396]]\n",
      "\n",
      " [[-0.12202158 -0.01127507]\n",
      "  [ 0.12301427 -0.00254198]\n",
      "  [-0.10187642 -0.18479692]\n",
      "  [-0.26198161  0.08168808]\n",
      "  [-0.15449019  0.17734981]\n",
      "  [-0.11592972 -0.03555814]]\n",
      "\n",
      " [[-0.11605831 -0.0319137 ]\n",
      "  [ 0.15400629  0.02233358]\n",
      "  [-0.06233519 -0.15406465]\n",
      "  [-0.2469681   0.10939408]\n",
      "  [-0.1789018   0.1432907 ]\n",
      "  [-0.15775643 -0.04350603]]]\n",
      "==============================\n",
      "[[[-0.02226454 -0.04573019]\n",
      "  [ 0.0657475  -0.02864933]\n",
      "  [-0.0438131   0.00853922]\n",
      "  [-0.07181516  0.0604394 ]\n",
      "  [-0.08508987  0.09737739]\n",
      "  [ 0.05444895 -0.06025686]]\n",
      "\n",
      " [[ 0.00934206 -0.06302911]\n",
      "  [ 0.09391693  0.05502656]\n",
      "  [ 0.08789979  0.07896547]\n",
      "  [ 0.01958     0.08437485]\n",
      "  [-0.0823015  -0.06080343]\n",
      "  [-0.09095454 -0.03493393]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[-0.09889558  0.06309229]\n",
      "  [ 0.04137147  0.04580143]\n",
      "  [ 0.05425407 -0.08519107]\n",
      "  [-0.02830685 -0.07682619]\n",
      "  [ 0.07262069  0.02465963]\n",
      "  [-0.0338204  -0.08728833]]]\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "momentum_n = 5\n",
    "np.random.seed(42)\n",
    "input_n = 6  #number of input nodes\n",
    "output_n = 2 #number of output nodes\n",
    "weights = .2*np.random.rand(input_n,output_n) - .1  #initial value of weights matrix\n",
    "print(weights)\n",
    "print('='*30)\n",
    "\n",
    "#weight_history = [] #I'm using a list of weights\n",
    "weight_history = np.zeros((momentum_n, input_n, output_n))\n",
    "test = ['p1', 'p2', 'p3', 'p4', 'p5']\n",
    "\n",
    "#print(weight_history)\n",
    "print('='*30)\n",
    "\n",
    "#We will do 6 simulated steps to mimic training on 6 samples.\n",
    "for i in range(6):\n",
    "\n",
    "    #generate simulated weight changes as if coming from backprop\n",
    "    simulated_changes = .2*np.random.rand(input_n,output_n) - .1\n",
    "  \n",
    "    print(simulated_changes)\n",
    "    #you fill in the rest\n",
    "    \"\"\"weight_history.append(simulated_changes)\n",
    "    \n",
    "    if(len(weight_history) > momentum_n):\n",
    "        del weight_history[0]\n",
    "        del test[0]\n",
    "    \"\"\"    \n",
    "    #weights = weights + (-current_change - (.5*past1 + .4*past2 + .3*past3 + .2*past4 + .1*past5)\n",
    "\n",
    "    \"\"\"\n",
    "    old = 0\n",
    "    for j in range(len(weight_history)):\n",
    "        influence = (momentum_n*.1) - (j*.1)\n",
    "        #print(influence)\n",
    "        old += influence * weight_history[-j]\n",
    "        \n",
    "        #print(influence)\n",
    "        #print(test[j])\n",
    "    wt = -simulated_changes - old\n",
    "    \n",
    "    weights = np.add(weights, wt)\n",
    "    #print(weights)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #accumulate weight change\n",
    "    weight_changes = [sample*z_deltas[i] for i in range(len(z_deltas))]\n",
    "    wt = np.transpose(weight_changes\n",
    "    weights = np.subtract(weights, wt)\n",
    "    \"\"\"\n",
    "    weights = weights + (simulated_changes - (np.multiply(.5, weight_history[4]),np.multiply(.4, weight_history[3]),np.multiply(.3, weight_history[2]),np.multiply(.2, weight_history[1]),np.multiply(.1, weight_history[0])))\n",
    "    \n",
    "    weight_history[len(weight_history)-1 - i] = simulated_changes       \n",
    "    if(len(weight_history) > momentum_n):\n",
    "        del weight_history[0]\n",
    "\n",
    "        \n",
    "    print(weights)\n",
    "    print('='*30)\n",
    "    print(weight_history)\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyASW333icpk"
   },
   "source": [
    "Here are my results in a text box.\n",
    "<pre>\n",
    "[[-0.02509198  0.09014286]\n",
    " [ 0.04639879  0.0197317 ]\n",
    " [-0.06879627 -0.0688011 ]\n",
    " [-0.08838328  0.07323523]\n",
    " [ 0.020223    0.04161452]\n",
    " [-0.0958831   0.09398197]]\n",
    "==============================\n",
    "[[[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]]\n",
    "==============================\n",
    "[[-0.0915805   0.14767504]\n",
    " [ 0.11003379  0.08305079]\n",
    " [-0.02964472 -0.07375238]\n",
    " [-0.07477228  0.1149894 ]\n",
    " [-0.00214758  0.11371574]\n",
    " [-0.05431203  0.1207096 ]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]]\n",
    "==============================\n",
    "[[-0.04955024  0.06187376]\n",
    " [ 0.13828154  0.04854436]\n",
    " [-0.06770341  0.01943318]\n",
    " [-0.10308675  0.16000749]\n",
    " [ 0.09602739 -0.01211198]\n",
    " [-0.16822397  0.04566632]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]]\n",
    "==============================\n",
    "[[ 0.01172942  0.14784406]\n",
    " [ 0.04594831  0.03660966]\n",
    " [ 0.00146978 -0.02297665]\n",
    " [-0.00465437  0.02849415]\n",
    " [ 0.10972479 -0.02856837]\n",
    " [-0.10063141  0.06180139]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]]\n",
    "==============================\n",
    "[[-0.0007191   0.1761948 ]\n",
    " [-0.07266192 -0.04225863]\n",
    " [-0.12857849 -0.13722301]\n",
    " [-0.06627522 -0.05383297]\n",
    " [ 0.13981959  0.06276776]\n",
    " [-0.00372652  0.11539563]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]\n",
    "\n",
    " [[ 0.00934206 -0.06302911]\n",
    "  [ 0.09391693  0.05502656]\n",
    "  [ 0.08789979  0.07896547]\n",
    "  [ 0.01958     0.08437485]\n",
    "  [-0.0823015  -0.06080343]\n",
    "  [-0.09095454 -0.03493393]]]\n",
    "==============================\n",
    "[[ 0.02124747  0.16382833]\n",
    " [-0.10745889 -0.00269356]\n",
    " [-0.07333787 -0.13288805]\n",
    " [-0.0181885  -0.0674587 ]\n",
    " [ 0.14283832 -0.03949667]\n",
    " [-0.09909215  0.17294928]]\n",
    "==============================\n",
    "[[[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]\n",
    "\n",
    " [[ 0.00934206 -0.06302911]\n",
    "  [ 0.09391693  0.05502656]\n",
    "  [ 0.08789979  0.07896547]\n",
    "  [ 0.01958     0.08437485]\n",
    "  [-0.0823015  -0.06080343]\n",
    "  [-0.09095454 -0.03493393]]\n",
    "\n",
    " [[-0.02226454 -0.04573019]\n",
    "  [ 0.0657475  -0.02864933]\n",
    "  [-0.0438131   0.00853922]\n",
    "  [-0.07181516  0.0604394 ]\n",
    "  [-0.08508987  0.09737739]\n",
    "  [ 0.05444895 -0.06025686]]]\n",
    "==============================\n",
    "[[ 0.10591608  0.03417346]\n",
    " [-0.0857124  -0.05016242]\n",
    " [-0.13723485 -0.02977743]\n",
    " [-0.04295321  0.08054189]\n",
    " [-0.03488198 -0.01929234]\n",
    " [-0.07125826  0.22700282]]\n",
    "==============================\n",
    "[[[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]\n",
    "\n",
    " [[ 0.00934206 -0.06302911]\n",
    "  [ 0.09391693  0.05502656]\n",
    "  [ 0.08789979  0.07896547]\n",
    "  [ 0.01958     0.08437485]\n",
    "  [-0.0823015  -0.06080343]\n",
    "  [-0.09095454 -0.03493393]]\n",
    "\n",
    " [[-0.02226454 -0.04573019]\n",
    "  [ 0.0657475  -0.02864933]\n",
    "  [-0.0438131   0.00853922]\n",
    "  [-0.07181516  0.0604394 ]\n",
    "  [-0.08508987  0.09737739]\n",
    "  [ 0.05444895 -0.06025686]]\n",
    "\n",
    " [[-0.09889558  0.06309229]\n",
    "  [ 0.04137147  0.04580143]\n",
    "  [ 0.05425407 -0.08519107]\n",
    "  [-0.02830685 -0.07682619]\n",
    "  [ 0.07262069  0.02465963]\n",
    "  [-0.0338204  -0.08728833]]]\n",
    "==============================\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8WmOrRhW85U"
   },
   "source": [
    "<h2>Question 4: Boosting</h2>\n",
    "  \n",
    "  There is a technique in machine learning called boosting. The general idea is that you identify the rows/samples that are giving you the most grief. You then concentrate on them. I'd like you to do a little exploration toward boosting. Please identify the samples in `predictions` (which you obtained in question 2)  that have a higher than average error. I'll break it into several steps.\n",
    "  <p>\n",
    "  1. As you know, `labels` is a list of a pair of values that have been one-hot encoded. So one of the pair is 0 and one of the pair is 1. Your prediction is also a pair of values. For each prediction, calculate the absolute difference between the 2 pairs, i.e., your prediction pair and the label pair. Keep your results in 2 different running sums. For instance, if your prediction was `array([0.66764488, 0.33235512])` and actual was `(0,1)`, you would have an error on 0 of `0.66764488` and an error on 1 of `(1  - 0.33235512)`. You would increment the sums for 0 errors and 1 errors accordingly.\n",
    "  2. Now set 2 variables to the average error being made on 0 and 1. I got the following values:<br>\n",
    "  ('average 0 error', 0.48547382904526587)<br>\n",
    "('average 1 error', 0.4875571623785993)\n",
    "  3. Find the predictions that are above average error for *both* 0 and 1. Store the indices of such predictions in bad_samples. Looking back to step 1, would a prediction of `array([0.66764488, 0.33235512])` and actual `(0,1)` be added to bad_samples? Yes. Both are in error beyond the average.\n",
    "  \n",
    "<p>\n",
    "  You can see my final results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "XGgq6gTgaSn3",
    "outputId": "b6dda205-afcb-45a4-a73d-28dba91f2b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4854738290452658 0.4854738290452659\n",
      "236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 6, 7, 8, 10, 11, 12]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For each prediction, calculate the absolute difference between the 2 pairs\n",
    "error0 = 0\n",
    "error1 = 0\n",
    "bad_samples = []\n",
    "for i in range(len(predictions)):\n",
    "    if(labels[i][0]==1):\n",
    "        error0 += (1 - predictions[i][0])\n",
    "        error1 += predictions[i][1]\n",
    "    \n",
    "    else:\n",
    "        error0 += predictions[i][0]\n",
    "        error1 += (1 - predictions[i][1])\n",
    "    \n",
    "#set 2 variables to the average error being made on 0 and 1\n",
    "avg0 = error0 / len(predictions)\n",
    "avg1 = error1 / len(predictions)\n",
    "print('%s %s' % (avg0, avg1))\n",
    "\n",
    "#Find the predictions that are above average error for both 0 and 1\n",
    "for i in range(len(predictions)):\n",
    "    if(labels[i][0]==1):\n",
    "        if(predictions[i][0] < avg0 and predictions[i][1] > avg1):\n",
    "            bad_samples.append(i)\n",
    "    else:\n",
    "        if((predictions[i][0] > avg0 and predictions[i][1] < avg1)):\n",
    "             bad_samples.append(i)\n",
    "\n",
    "print(len(bad_samples))\n",
    "bad_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNg9bZtebV2y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practice_final_inclass_w19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
