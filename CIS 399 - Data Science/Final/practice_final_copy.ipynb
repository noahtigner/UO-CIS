{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh4uU7eob-bc"
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Practice Final Exam\n",
    "</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQcquKEsbB0b"
   },
   "outputs": [],
   "source": [
    "!rm library_w19_deep_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Wl7zyWU-bG55",
    "outputId": "578fc53a-5adf-4130-ce6d-5d65f8fc4f9b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-763edb0b-0254-45db-9fab-d9a3a2093a43\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-763edb0b-0254-45db-9fab-d9a3a2093a43\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving library_w19_deep_2.py to library_w19_deep_2.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'library_w19_deep_2.py': b\"import pandas as pd\\nimport numpy as np\\n\\ndef sigmoid(x):  \\n    return 1/(1+np.exp(-x))\\n\\ndef mse(z,y):\\n  return (z-y)**2\\n\\ndef mse_der(z,y):\\n  return z-y\\n\\ndef sigmoid_der(x):  \\n    return sigmoid(x)*(1-sigmoid(x))\\n\\ndef ann_simple(all_samples, labels, weights, bias, hypers={}):\\n  \\n  '''\\n  Can build an ANN with n input nodes and one output node.\\n  Uses sigmoid and mse.\\n  '''\\n  \\n  input_n = all_samples.shape[1]  #number of inputs in each sample\\n  \\n  assert weights.shape == (input_n,1), 'weights needs to have same shape as sample'\\n  assert all_samples.shape[0] >= 1, 'all_samples must represent 1 or more samples'\\n  assert bias.shape == (1,) , 'a single bias weight for output node'\\n  assert labels.shape[1] == 1, 'actual value for the 1 output node'\\n  assert labels.shape[0] == all_samples.shape[0], 'labels must match up with samples'\\n  \\n  hyper_keys = [*hypers]  #fails on 2.7\\n  target_set = set(['epochs', 'cost-reporting', 'learning-rate'])  #might add more later\\n  diff_set = set(hyper_keys) - target_set\\n  if diff_set: print('WARNING: unrecognized hyper parameters ' + str(diff_set))\\n\\n  max_epochs = hypers['epochs'] if 'epochs' in hypers else 100\\n  cost_reporting = hypers['cost-reporting'] if 'cost-reporting' in hypers else 100  #how often to report epoch cost\\n  alpha = hypers['learning-rate'] if 'learning-rate' in hypers else .05\\n  \\n  cost_accumulator = [0, 0]  #[count, sum] use to print out costs now and then\\n  \\n  for i in range(max_epochs):\\n\\n    #Go through each sample forward and backward\\n    for j in range(len(all_samples)):\\n\\n\\n      #do forward propogation\\n      sample = np.expand_dims(all_samples[j], axis=1) #transform to match up with weight shape\\n      XW = np.multiply(sample, weights)\\n      XW_sum = np.sum(XW, axis=0)\\n      raw_output = XW_sum + bias\\n      z = sigmoid(raw_output)\\n\\n      #compute error\\n      cost = mse(z, labels[j])\\n      cost_accumulator[0] += 1\\n      cost_accumulator[1] += cost\\n\\n      #back propogation\\n      mse_deriv_value = mse_der(z, labels[j])\\n      sigmoid_deriv_value = sigmoid_der(raw_output)\\n      z_delta = mse_deriv_value * sigmoid_deriv_value\\n\\n      #update weights\\n      for k in range(len(weights)):\\n        #print(('before weight', weights[k]))\\n        weights[k][0] -= alpha * all_samples[j][k] * z_delta\\n        #print(('after weight', weights[k]))\\n\\n      #update bias\\n      bias -=  1.0*z_delta\\n\\n    #print ith cost value\\n    if i%cost_reporting == 0:\\n      average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\n      print((i,average_cost))\\n      cost_accumulator = [0, 0]  #reset\\n  #end epoch loop\\n\\n  if cost_accumulator[0]:\\n    average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\n    print((max_epochs,average_cost))\\n\\n  return (weights,bias)  #don't lose these! We worked hard to get them.\\n\\n#build/train a new ann starting with known random seeds\\ndef from_scratch(samples, labels, hypers):\\n  \\n  input_n = samples.shape[1]\\n  \\n  #reset weights to initial values. Seed of 42 guarantees same random values\\n  np.random.seed(42)\\n  weights = np.random.rand(input_n,1)\\n  bias = np.random.rand(1)\\n  \\n  return ann_simple(samples, labels, weights, bias, hypers)\\n\\ndef ann_predictor(sample, weights, bias):\\n  \\n  s2 = np.expand_dims(sample, axis=1)\\n  XW = np.multiply(s2, weights)\\n  XW_sum = np.sum(XW, axis=0)\\n  raw_output = XW_sum + bias\\n  z = sigmoid(raw_output)\\n\\n  return 1 if z > .5 else 0\\n\\ndef ann_tester(samples, labels, weights, bias):\\n  weights = np.array(weights)\\n  bias = np.array(bias)\\n  \\n  predictions = [ann_predictor(s, weights, bias) for s in samples]\\n  zipped = list(zip(predictions, labels))\\n  \\n  return (zipped.count((1,1)) + zipped.count((0,0)))/len(zipped)\\n\\n#from assignment notebook\\n\\ndef ann_simple_batch(all_samples, labels, weights, bias, hypers={}):\\n  \\n  '''\\n  Can build an ANN with n input nodes and one output node.\\n  Uses sigmoid and mse.\\n  '''\\n  \\n  input_n = all_samples.shape[1]  #number of inputs in each sample\\n  \\n  assert weights.shape == (input_n,1), 'weights needs to have same shape as sample'\\n  assert all_samples.shape[0] >= 1, 'all_samples must represent 1 or more samples'\\n  assert bias.shape == (1,) , 'a single bias weight for output node'\\n  assert labels.shape[1] == 1, 'actual value for the 1 output node'\\n  assert labels.shape[0] == all_samples.shape[0], 'labels must match up with samples'\\n  \\n  hyper_keys = [*hypers]  #fails on 2.7\\n  target_set = set(['epochs', 'cost-reporting', 'learning-rate', 'batch-size'])  #might add more later\\n  diff_set = set(hyper_keys) - target_set\\n  if diff_set: print('WARNING: unrecognized hyper parameters ' + str(diff_set))\\n\\n  max_epochs = hypers['epochs'] if 'epochs' in hypers else 100\\n  cost_reporting = hypers['cost-reporting'] if 'cost-reporting' in hypers else 100  #how often to report epoch cost\\n  alpha = hypers['learning-rate'] if 'learning-rate' in hypers else .05  #learning rate\\n  batch_n = min(hypers['batch-size'],len(all_samples))  if 'batch-size' in hypers else 1  #batch size\\n   \\n  cost_accumulator = [0, 0]  #[count, sum] use to print out costs now and then\\n\\n  temp_weights = np.zeros(weights.shape)  #reset accumulators\\n  temp_bias = np.zeros(bias.shape)\\n  \\n  for i in range(max_epochs):\\n    \\n    sample_count = 1\\n\\n    #Go through each sample forward and backward\\n    for j in range(len(all_samples)):\\n\\n\\n      #do forward propogation\\n      sample = np.expand_dims(all_samples[j], axis=1) #transform to match up with weight shape\\n      XW = np.multiply(sample, weights)\\n      XW_sum = np.sum(XW, axis=0)\\n      raw_output = XW_sum + bias\\n      z = sigmoid(raw_output)\\n\\n      #compute error\\n      cost = mse(z, labels[j])\\n      cost_accumulator[0] += 1\\n      cost_accumulator[1] += cost\\n\\n      #back propogation\\n      mse_deriv_value = mse_der(z, labels[j])\\n      sigmoid_deriv_value = sigmoid_der(raw_output)\\n      delta_z = mse_deriv_value * sigmoid_deriv_value\\n      \\n      #accumulate changes\\n      for k in range(len(weights)):\\n            temp_weights[k][0] += alpha * all_samples[j][k] * delta_z\\n      temp_bias +=  1.0*delta_z\\n      \\n      if sample_count == batch_n:\\n        #batch is finished so update weights\\n        for k in range(len(weights)):\\n          weights[k][0] -= temp_weights[k][0]\\n        bias -=  temp_bias\\n        sample_count = 1  #reset batch counter\\n        temp_weights = np.zeros(weights.shape)  #reset accumulators\\n        temp_bias = np.zeros(bias.shape)\\n      else:\\n        #batch not finished so keep accumulating\\n        sample_count += 1\\n        \\n    #end sample loop\\n    \\n    #Add in whatever is left in temp_weights (could be 0)\\n    for k in range(len(weights)):\\n      weights[k][0] -= temp_weights[k][0]\\n      #print(('after weight', weights[k]))\\n    bias -=  temp_bias\\n\\n    #print ith cost value\\n    if i%cost_reporting == 0:\\n      average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\n      print((i,average_cost))\\n      cost_accumulator = [0, 0]  #reset\\n      \\n  #end epoch loop\\n\\n  if cost_accumulator[0]:\\n    average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\n    print((max_epochs,average_cost))\\n  \\n  return (weights,bias)  #don't lose these!\\n\\n\\ndef from_scratch_batch(samples, labels, hypers):\\n  \\n  input_n = samples.shape[1]\\n  \\n  #reset weights to initial values. Seed of 42 guarantees same random values\\n  np.random.seed(42)\\n  weights = np.random.rand(input_n,1)\\n  bias = np.random.rand(1)\\n  \\n  return ann_simple_batch(samples, labels, weights, bias, hypers)\\n\\n#helpers I defined for myself\\n\\ndef table_to_net_table(table, input_columns, label_column):\\n  ann_table = table[input_columns+[label_column]].copy()\\n  ann_table = ann_table.dropna()  #drop all rows with a NaN\\n  \\n  for column in input_columns:\\n    unique_vals = set(ann_table[column].unique())\\n    \\n    if unique_vals == set([0,1]): continue  #column is in good shape\\n      \\n    if len(unique_vals) == 2:\\n      list_unique = list(unique_vals)\\n      ann_table[column] = ann_table[column].replace({list_unique[1]: 1, list_unique[0]: 0})  #change 2 values to 0 and 1\\n      continue\\n    \\n    if ann_table[column].dtypes == 'float64' or ann_table[column].dtypes == 'int64':\\n      if ann_table[column].min() >= 0 and ann_table[column].max() <= 1: continue  #already normalized\\n      ann_table[column+'_normed'] = ann_table[column]/ann_table[column].max()\\n      ann_table = ann_table.drop([column], axis=1)\\n      continue\\n      \\n    print('Error: no wrangling rule found for column: ' + column + ' with dtype ' + str(ann_table[column].dtypes))\\n    return None\\n  \\n  return ann_table\\n\\ndef net_table_to_input(net_table, label_column):\\n  labels = net_table[label_column]\\n  labels = labels.values\\n  labels = np.expand_dims(labels, axis=1)\\n  net_table = net_table.drop([label_column], axis=1)  #Have the labels so don't need column anymore\\n  \\n  feature_set = net_table.values.tolist()\\n  feature_set = np.asarray(feature_set)  #convert to numpy matrix\\n  \\n  return (feature_set, labels)\\n\\n\\n############### module 9\\n\\ndef ann_flex(all_samples, all_labels, weights, biases, hypers={}):\\n  \\n  '''\\n  Can build an ANN with n input nodes and m output nodes.\\n  Uses sigmoid and mse.\\n  '''\\n  \\n  input_n = all_samples.shape[1]  #number of inputs in each sample\\n  output_n = all_labels.shape[1]      #number of outputs in each label\\n  \\n  assert weights.shape == (input_n,output_n), 'weights needs to have same shape as sample'\\n  assert all_samples.shape[0] >= 1, 'all_samples must represent 1 or more samples'\\n  assert biases.shape == (output_n,) , 'a bias weight for each output node'\\n  assert all_labels.shape[0] == all_samples.shape[0], 'labels must match up with samples'\\n  \\n  hyper_keys = [*hypers]  #fails on 2.7\\n  target_set = set(['epochs', 'cost-reporting', 'learning-rate'])  #might add more later\\n  diff_set = set(hyper_keys) - target_set\\n  if diff_set: print('WARNING: unrecognized hyper parameters ' + str(diff_set))\\n\\n  max_epochs = hypers['epochs'] if 'epochs' in hypers else 100\\n  cost_reporting = hypers['cost-reporting'] if 'cost-reporting' in hypers else 100  #how often to report epoch cost\\n  alpha = hypers['learning-rate'] if 'learning-rate' in hypers else .05\\n  \\n  fsig = np.vectorize(sigmoid)\\n  fsig_der = np.vectorize(sigmoid_der)\\n  \\n  cost_accumulator = [0, 0]  #[count, sum] use to print out costs now and then\\n  \\n  for i in range(max_epochs):\\n\\n    #Go through each sample forward and backward\\n    for j,sample in enumerate(all_samples):\\n      \\n      label = all_labels[j] #label that goes with this sample\\n      \\n      #do forward propogation\\n      raw = sample.dot(weights)\\n      full_raw = np.add(raw, biases)\\n      final_output = fsig(full_raw) #an array of values for output nodes\\n\\n      #compute error\\n      costs = np.array([mse(a,b) for (a,b) in zip(final_output, label)])  #will get m costs for the m output nodes\\n      \\n      cost_accumulator[0] += 1\\n      cost_accumulator[1] += sum(costs)  #sum to get total cost\\n\\n      #back propogation\\n      mse_deriv_values = np.array([mse_der(a,b) for (a,b) in zip(final_output, label)])\\n      sigmoid_deriv_values = fsig_der(full_raw)\\n      z_deltas = mse_deriv_values * sigmoid_deriv_values\\n      \\n      #update weights\\n      weight_changes = [sample*z_deltas[i] for i in range(len(z_deltas))]\\n      wt = np.transpose(weight_changes)\\n      weights = np.subtract(weights, wt)\\n      \\n      #update biases\\n      biases = np.subtract(biases, 1.0*z_deltas)\\n\\n    #print ith cost value\\n    if i%cost_reporting == 0:\\n      average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\n      print((i,average_cost))\\n      cost_accumulator = [0, 0]  #reset\\n  #end epoch loop\\n\\n  if cost_accumulator[0]:\\n    average_cost = cost_accumulator[1]/cost_accumulator[0]  #really mse where n is cost_reporting epochs\\n    print((max_epochs,average_cost))\\n\\n  return (weights,biases)  #don't lose these! We worked hard to get them.\\n\\n\\ndef from_scratch_flex(samples, labels, hypers):\\n  \\n  input_n = samples.shape[1]\\n  output_n = labels.shape[1]\\n  \\n  #reset weights to initial values. Seed of 42 guarantees same random values\\n  np.random.seed(42)\\n  weights = .2*np.random.rand(input_n,output_n) - .1\\n  biases = np.random.rand(output_n)\\n  \\n  return ann_flex(samples, labels, weights, biases, hypers)\\n\\n\\ndef ann_flex_predictor(sample, weights, biases):\\n  fsig = np.vectorize(sigmoid)\\n  \\n  #do forward propogation\\n  raw = sample.dot(weights)\\n  full_raw = np.add(raw, biases)\\n  final_output = fsig(full_raw) #an array of values for output nodes\\n\\n  return final_output\\n\\n\\ndef ann_flex_tester(samples,  weights, biases):\\n  predictions = [ann_flex_predictor(s, weights, biases) for s in samples] #an array of arrays\\n  \\n  return predictions\\n\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXW0jSlfipja"
   },
   "source": [
    "<h2>Question 1: import `ann_flex` and others (15 points)</h2>\n",
    "\n",
    "This is from your take home portion. If you successfully bring in the flex functions, you get the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "G8rLH27bbevq",
    "outputId": "97993c5b-aaf9-4dfa-cea2-1d7fcc6d30f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann_flex\t ann_flex_predictor\t ann_flex_tester\t ann_predictor\t ann_simple\t ann_simple_batch\t ann_tester\t from_scratch\t from_scratch_batch\t \n",
      "from_scratch_flex\t mse\t mse_der\t net_table_to_input\t sigmoid\t sigmoid_der\t table_to_net_table\t \n"
     ]
    }
   ],
   "source": [
    "from library_w19_deep_2 import *\n",
    "\n",
    "%who function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn2536_DVRFX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bbwugquCg4lA",
    "outputId": "a97de39d-2b19-42ab-8460-e3f8defc6d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IP18ATeIua5"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/My Drive/class_tables/loan_table_week4.csv', 'r') as f:\n",
    "  loan_table = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "2e-PADzZhEO0",
    "outputId": "f3b16763-054f-49a2-9707-f7b3d29735c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
       "       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
       "       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status',\n",
       "       'no_lam', 'filled_lam', 'pa_Rural', 'pa_Semiurban', 'pa_Urban',\n",
       "       'pa_nan', 'lam_bin', 'lam_Low', 'lam_Average', 'lam_High', 'ch_bad',\n",
       "       'ch_good', 'ch_nan', 'apin_binned', 'apin_low', 'apin_average',\n",
       "       'apin_high', 'apin_nan', 'dep_0', 'dep_1', 'dep_2', 'dep_3+',\n",
       "       'dep_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "an3i3EhpkywZ"
   },
   "source": [
    "<h2>Question 2: predict a new column in `loan_table`</h2>\n",
    "<p>\n",
    "I am going to give you the goal and let you fill in the steps. The goal is to use `ann_flex` to train a model for predicting a new column in `loan_table`. Here are specifics:\n",
    "  \n",
    "  1.  Drop all rows with a `NaN` in any column. I was left with 480 rows when I did this.\n",
    "  1. Create a new column `coap_binned` that has 2 values, `Low and High`. Use `qcut` to get the 2 bins from `CoapplicantIncome`.\n",
    "  2. For your sample, use these columns: 'lam_normed, 'pa_Rural', 'pa_Semiurban', 'pa_Urban', 'ch_bad', 'ch_good'. You will have create `lam_normed`, which is the normed version of `LoanAmount`. You can see you will need 6 input nodes for your ANN.\n",
    "  4. I would like 2 output nodes, corresponding to the one-hot encoding of `coap_binned`.\n",
    "  \n",
    "Match my intermediate results shown below. And compute the accuracy against the training table and match my accuracy.\n",
    "  <pre>\n",
    "feature_set[0]\n",
    "array([1.        , 0.        , 0.        , 0.        , 1.        , 0.21333333])\n",
    "\n",
    "(weights, biases) = from_scratch_flex(feature_set, labels, hypers={'epochs':3000, 'cost-reporting': 1000})\n",
    "(0, 0.5440918077466073)\n",
    "(1000, 0.5467307425352096)\n",
    "(2000, 0.546736672684983)\n",
    "(3000, 0.546736672684985)\n",
    "\n",
    "predictions = ann_flex_tester(feature_set, weights, biases)\n",
    "predictions[:10]\n",
    "[array([0.66764488, 0.33235512]),\n",
    " array([0.8547766, 0.1452234]),\n",
    " array([0.81404568, 0.18595432]),\n",
    " array([0.79598459, 0.20401541]),\n",
    " array([0.66163692, 0.33836308]),\n",
    " array([0.83390732, 0.16609268]),\n",
    " array([0.77282866, 0.22717134]),\n",
    " array([0.77089266, 0.22910734]),\n",
    " array([0.7522263, 0.2477737]),\n",
    " array([0.85203315, 0.14796685])]\n",
    " \n",
    " accuracy: 0.50625\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ZzWKRykGM5Vk",
    "outputId": "c3bcc180-1824-440b-ab1b-4492f61d2571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.21333333])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_set.shape)\n",
    "feature_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "GXCxriyz1Azc",
    "outputId": "9e95119c-3601-459a-dc92-3266d1058025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.5440918077466073)\n",
      "(1000, 0.5467307425352096)\n",
      "(2000, 0.546736672684983)\n",
      "(3000, 0.546736672684985)\n"
     ]
    }
   ],
   "source": [
    "(weights, biases) = from_scratch_flex(feature_set, labels, hypers={'epochs':3000, 'cost-reporting': 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MF9baNMQ1ikg",
    "outputId": "09cfd359-f2eb-4612-d63b-60ba4fda9a92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66764488, 0.33235512])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_flex_predictor(feature_set[0], weights, biases)  #try it on first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "OvqdDxzQ16_k",
    "outputId": "0c02f8bd-c3f6-45c4-cb46-45bfa3d4cc87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.66764488, 0.33235512]),\n",
       " array([0.8547766, 0.1452234]),\n",
       " array([0.81404568, 0.18595432]),\n",
       " array([0.79598459, 0.20401541]),\n",
       " array([0.66163692, 0.33836308]),\n",
       " array([0.83390732, 0.16609268]),\n",
       " array([0.77282866, 0.22717134]),\n",
       " array([0.77089266, 0.22910734]),\n",
       " array([0.7522263, 0.2477737]),\n",
       " array([0.85203315, 0.14796685])]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ann_flex_tester(feature_set, weights, biases)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUVqmwh3g4vv"
   },
   "source": [
    "Note: I used `pair[0]>pair[1]` to get `(1,0)` else `(0,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U7-WTDTi3KYT",
    "outputId": "94f0e400-6778-44e8-f14b-1b40efbf6d4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50625"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kij1-pe-gXOV"
   },
   "source": [
    "<h2>Question 3: Momentum</h2>\n",
    "\n",
    "One technique for avoiding big weight-change swings is to use the weight-change history to temper the current weight-change. This is called using momentum (jargon). I want you to explore the momentum algorithm without trying to modify `ann_flex`. Once you are happy with your algorithm, you could consider building a new `ann_mo` function that uses your weight change algorithm. But that is not part of the exam.\n",
    "<p>\n",
    "  To be able to test the alogirthm separately, we will need to set up a bit of simulation. We will simulate pieces of `ann_flex`. In particular, we will simulate obtaining a weight change matrix. We will just randomly make one up,over and over, for testing. Once we have such a matrix, our momentum algorithm can kick in.\n",
    "<p>\n",
    "  So what is this momentum algorithm all about? The general idea is to interpose a momentum calculation inbetween computing a weight-change with back-propogation and actually changing the weights. The approach is keep a record of the weight changes we have computed in the past. How for in the past should we go?  We need a new hyper-parameter `momentum_n`. It says how far back in history we should keep a record. Assume this ranges from 0 (no history kept) to 5 (5 past weight changes kept).\n",
    "  <p>\n",
    "To compute the actual change to the weights, we use this formula. You can see as we move further into the past, we give less influence to past changes.\n",
    "    <p>\n",
    "      `weights = weights + (-current_change - (.5*past1 + .4*past2 + .3*past3 + .2*past4 + .1*past5)`.\n",
    "    <p>\n",
    "Here, `current_change` is the weight changes we obtained from back-propogation. Then `past1` is the actual change calculated 1 step ago (i.e., for the sample before this one). I say \"actual\" to mean the raw weight change obtained by back-propogation and before modified by momentum. `Past2` is from 2 steps ago, etc. This assumes `momentum_n` is set to 5. If it is smaller, then we will lop off the right terms in the formula. If it is 0, then no modificaiton is made to current_change.\n",
    "<p>\n",
    "I hope you infer that you will have to keep track of `past1, past2`, etc. I used a list of weight matrices. If `momentum_n` is 5, then I had a list of 5 weight matrices. I chose to treat the first item in the list as oldest. After calculating the weight change using the formula above, I slid everything in the list left and put current_change into the right most slot. You do not have to do it this way! There are likely more elegant ways of doing it.\n",
    "  <p>\n",
    "    I'll give you pieces of the simulation loop below. I print out the values of `weights` and my history list for every new sample. You should be able to match my results. Notice the first thing printed are the weights and my history list prior to starting the simulation loop. Then I print them at the end of each loop iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4956
    },
    "colab_type": "code",
    "id": "403xa6LMeD4J",
    "outputId": "aacafa5f-84cd-47ba-820a-3e78b426b6a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02509198  0.09014286]\n",
      " [ 0.04639879  0.0197317 ]\n",
      " [-0.06879627 -0.0688011 ]\n",
      " [-0.08838328  0.07323523]\n",
      " [ 0.020223    0.04161452]\n",
      " [-0.0958831   0.09398197]]\n",
      "==============================\n",
      "[[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n",
      "==============================\n",
      "[[-0.0915805   0.14767504]\n",
      " [ 0.11003379  0.08305079]\n",
      " [-0.02964472 -0.07375238]\n",
      " [-0.07477228  0.1149894 ]\n",
      " [-0.00214758  0.11371574]\n",
      " [-0.05431203  0.1207096 ]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]]\n",
      "==============================\n",
      "[[-0.04955024  0.06187376]\n",
      " [ 0.13828154  0.04854436]\n",
      " [-0.06770341  0.01943318]\n",
      " [-0.10308675  0.16000749]\n",
      " [ 0.09602739 -0.01211198]\n",
      " [-0.16822397  0.04566632]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]]\n",
      "==============================\n",
      "[[ 0.01172942  0.14784406]\n",
      " [ 0.04594831  0.03660966]\n",
      " [ 0.00146978 -0.02297665]\n",
      " [-0.00465437  0.02849415]\n",
      " [ 0.10972479 -0.02856837]\n",
      " [-0.10063141  0.06180139]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]]\n",
      "==============================\n",
      "[[-0.0007191   0.1761948 ]\n",
      " [-0.07266192 -0.04225863]\n",
      " [-0.12857849 -0.13722301]\n",
      " [-0.06627522 -0.05383297]\n",
      " [ 0.13981959  0.06276776]\n",
      " [-0.00372652  0.11539563]]\n",
      "==============================\n",
      "[[[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[ 0.00934206 -0.06302911]\n",
      "  [ 0.09391693  0.05502656]\n",
      "  [ 0.08789979  0.07896547]\n",
      "  [ 0.01958     0.08437485]\n",
      "  [-0.0823015  -0.06080343]\n",
      "  [-0.09095454 -0.03493393]]]\n",
      "==============================\n",
      "[[ 0.02124747  0.16382833]\n",
      " [-0.10745889 -0.00269356]\n",
      " [-0.07333787 -0.13288805]\n",
      " [-0.0181885  -0.0674587 ]\n",
      " [ 0.14283832 -0.03949667]\n",
      " [-0.09909215  0.17294928]]\n",
      "==============================\n",
      "[[[ 0.06648853 -0.05753218]\n",
      "  [-0.06363501 -0.0633191 ]\n",
      "  [-0.03915155  0.00495129]\n",
      "  [-0.013611   -0.04175417]\n",
      "  [ 0.02237058 -0.07210123]\n",
      "  [-0.04157107 -0.02672763]]\n",
      "\n",
      " [[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[ 0.00934206 -0.06302911]\n",
      "  [ 0.09391693  0.05502656]\n",
      "  [ 0.08789979  0.07896547]\n",
      "  [ 0.01958     0.08437485]\n",
      "  [-0.0823015  -0.06080343]\n",
      "  [-0.09095454 -0.03493393]]\n",
      "\n",
      " [[-0.02226454 -0.04573019]\n",
      "  [ 0.0657475  -0.02864933]\n",
      "  [-0.0438131   0.00853922]\n",
      "  [-0.07181516  0.0604394 ]\n",
      "  [-0.08508987  0.09737739]\n",
      "  [ 0.05444895 -0.06025686]]]\n",
      "==============================\n",
      "[[ 0.10591608  0.03417346]\n",
      " [-0.0857124  -0.05016242]\n",
      " [-0.13723485 -0.02977743]\n",
      " [-0.04295321  0.08054189]\n",
      " [-0.03488198 -0.01929234]\n",
      " [-0.07125826  0.22700282]]\n",
      "==============================\n",
      "[[[-0.008786    0.05703519]\n",
      "  [-0.06006524  0.00284689]\n",
      "  [ 0.01848291 -0.09070992]\n",
      "  [ 0.02150897 -0.06589518]\n",
      "  [-0.08698968  0.08977711]\n",
      "  [ 0.09312641  0.06167947]]\n",
      "\n",
      " [[-0.03907725 -0.08046558]\n",
      "  [ 0.03684661 -0.0119695 ]\n",
      "  [-0.07559235 -0.00096462]\n",
      "  [-0.0931223   0.08186408]\n",
      "  [-0.048244    0.03250446]\n",
      "  [-0.03765778  0.0040136 ]]\n",
      "\n",
      " [[ 0.00934206 -0.06302911]\n",
      "  [ 0.09391693  0.05502656]\n",
      "  [ 0.08789979  0.07896547]\n",
      "  [ 0.01958     0.08437485]\n",
      "  [-0.0823015  -0.06080343]\n",
      "  [-0.09095454 -0.03493393]]\n",
      "\n",
      " [[-0.02226454 -0.04573019]\n",
      "  [ 0.0657475  -0.02864933]\n",
      "  [-0.0438131   0.00853922]\n",
      "  [-0.07181516  0.0604394 ]\n",
      "  [-0.08508987  0.09737739]\n",
      "  [ 0.05444895 -0.06025686]]\n",
      "\n",
      " [[-0.09889558  0.06309229]\n",
      "  [ 0.04137147  0.04580143]\n",
      "  [ 0.05425407 -0.08519107]\n",
      "  [-0.02830685 -0.07682619]\n",
      "  [ 0.07262069  0.02465963]\n",
      "  [-0.0338204  -0.08728833]]]\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "momentum_n = 5\n",
    "np.random.seed(42)\n",
    "input_n = 6  #number of input nodes\n",
    "output_n = 2 #number of output nodes\n",
    "weights = .2*np.random.rand(input_n,output_n) - .1  #initial value of weights matrix\n",
    "print(weights)\n",
    "print('='*30)\n",
    "\n",
    "weight_history = #I'm using a list of weights\n",
    "\n",
    "print(weight_history)\n",
    "print('='*30)\n",
    "\n",
    "#We will do 6 simulated steps to mimic training on 6 samples.\n",
    "for i in range(6):\n",
    "  \n",
    "  #generate simulated weight changes as if coming from backprop\n",
    "  simulated_changes = .2*np.random.rand(input_n,output_n) - .1\n",
    "  \n",
    "  #you fill in the rest\n",
    "  \n",
    "  print(weights)\n",
    "  print('='*30)\n",
    "  print(weight_history)\n",
    "  print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyASW333icpk"
   },
   "source": [
    "Here are my results in a text box.\n",
    "<pre>\n",
    "[[-0.02509198  0.09014286]\n",
    " [ 0.04639879  0.0197317 ]\n",
    " [-0.06879627 -0.0688011 ]\n",
    " [-0.08838328  0.07323523]\n",
    " [ 0.020223    0.04161452]\n",
    " [-0.0958831   0.09398197]]\n",
    "==============================\n",
    "[[[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]\n",
    "\n",
    " [[0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]\n",
    "  [0. 0.]]]\n",
    "==============================\n",
    "[[-0.0915805   0.14767504]\n",
    " [ 0.11003379  0.08305079]\n",
    " [-0.02964472 -0.07375238]\n",
    " [-0.07477228  0.1149894 ]\n",
    " [-0.00214758  0.11371574]\n",
    " [-0.05431203  0.1207096 ]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]]\n",
    "==============================\n",
    "[[-0.04955024  0.06187376]\n",
    " [ 0.13828154  0.04854436]\n",
    " [-0.06770341  0.01943318]\n",
    " [-0.10308675  0.16000749]\n",
    " [ 0.09602739 -0.01211198]\n",
    " [-0.16822397  0.04566632]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]]\n",
    "==============================\n",
    "[[ 0.01172942  0.14784406]\n",
    " [ 0.04594831  0.03660966]\n",
    " [ 0.00146978 -0.02297665]\n",
    " [-0.00465437  0.02849415]\n",
    " [ 0.10972479 -0.02856837]\n",
    " [-0.10063141  0.06180139]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]]\n",
    "==============================\n",
    "[[-0.0007191   0.1761948 ]\n",
    " [-0.07266192 -0.04225863]\n",
    " [-0.12857849 -0.13722301]\n",
    " [-0.06627522 -0.05383297]\n",
    " [ 0.13981959  0.06276776]\n",
    " [-0.00372652  0.11539563]]\n",
    "==============================\n",
    "[[[ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]\n",
    "  [ 0.          0.        ]]\n",
    "\n",
    " [[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]\n",
    "\n",
    " [[ 0.00934206 -0.06302911]\n",
    "  [ 0.09391693  0.05502656]\n",
    "  [ 0.08789979  0.07896547]\n",
    "  [ 0.01958     0.08437485]\n",
    "  [-0.0823015  -0.06080343]\n",
    "  [-0.09095454 -0.03493393]]]\n",
    "==============================\n",
    "[[ 0.02124747  0.16382833]\n",
    " [-0.10745889 -0.00269356]\n",
    " [-0.07333787 -0.13288805]\n",
    " [-0.0181885  -0.0674587 ]\n",
    " [ 0.14283832 -0.03949667]\n",
    " [-0.09909215  0.17294928]]\n",
    "==============================\n",
    "[[[ 0.06648853 -0.05753218]\n",
    "  [-0.06363501 -0.0633191 ]\n",
    "  [-0.03915155  0.00495129]\n",
    "  [-0.013611   -0.04175417]\n",
    "  [ 0.02237058 -0.07210123]\n",
    "  [-0.04157107 -0.02672763]]\n",
    "\n",
    " [[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]\n",
    "\n",
    " [[ 0.00934206 -0.06302911]\n",
    "  [ 0.09391693  0.05502656]\n",
    "  [ 0.08789979  0.07896547]\n",
    "  [ 0.01958     0.08437485]\n",
    "  [-0.0823015  -0.06080343]\n",
    "  [-0.09095454 -0.03493393]]\n",
    "\n",
    " [[-0.02226454 -0.04573019]\n",
    "  [ 0.0657475  -0.02864933]\n",
    "  [-0.0438131   0.00853922]\n",
    "  [-0.07181516  0.0604394 ]\n",
    "  [-0.08508987  0.09737739]\n",
    "  [ 0.05444895 -0.06025686]]]\n",
    "==============================\n",
    "[[ 0.10591608  0.03417346]\n",
    " [-0.0857124  -0.05016242]\n",
    " [-0.13723485 -0.02977743]\n",
    " [-0.04295321  0.08054189]\n",
    " [-0.03488198 -0.01929234]\n",
    " [-0.07125826  0.22700282]]\n",
    "==============================\n",
    "[[[-0.008786    0.05703519]\n",
    "  [-0.06006524  0.00284689]\n",
    "  [ 0.01848291 -0.09070992]\n",
    "  [ 0.02150897 -0.06589518]\n",
    "  [-0.08698968  0.08977711]\n",
    "  [ 0.09312641  0.06167947]]\n",
    "\n",
    " [[-0.03907725 -0.08046558]\n",
    "  [ 0.03684661 -0.0119695 ]\n",
    "  [-0.07559235 -0.00096462]\n",
    "  [-0.0931223   0.08186408]\n",
    "  [-0.048244    0.03250446]\n",
    "  [-0.03765778  0.0040136 ]]\n",
    "\n",
    " [[ 0.00934206 -0.06302911]\n",
    "  [ 0.09391693  0.05502656]\n",
    "  [ 0.08789979  0.07896547]\n",
    "  [ 0.01958     0.08437485]\n",
    "  [-0.0823015  -0.06080343]\n",
    "  [-0.09095454 -0.03493393]]\n",
    "\n",
    " [[-0.02226454 -0.04573019]\n",
    "  [ 0.0657475  -0.02864933]\n",
    "  [-0.0438131   0.00853922]\n",
    "  [-0.07181516  0.0604394 ]\n",
    "  [-0.08508987  0.09737739]\n",
    "  [ 0.05444895 -0.06025686]]\n",
    "\n",
    " [[-0.09889558  0.06309229]\n",
    "  [ 0.04137147  0.04580143]\n",
    "  [ 0.05425407 -0.08519107]\n",
    "  [-0.02830685 -0.07682619]\n",
    "  [ 0.07262069  0.02465963]\n",
    "  [-0.0338204  -0.08728833]]]\n",
    "==============================\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8WmOrRhW85U"
   },
   "source": [
    "<h2>Question 4: Boosting</h2>\n",
    "  \n",
    "  There is a technique in machine learning called boosting. The general idea is that you identify the rows/samples that are giving you the most grief. You then concentrate on them. I'd like you to do a little exploration toward boosting. Please identify the samples in `predictions` (which you obtained in question 2)  that have a higher than average error. I'll break it into several steps.\n",
    "  <p>\n",
    "  1. As you know, `labels` is a list of a pair of values that have been one-hot encoded. So one of the pair is 0 and one of the pair is 1. Your prediction is also a pair of values. For each prediction, calculate the absolute difference between the 2 pairs, i.e., your prediction pair and the label pair. Keep your results in 2 different running sums. For instance, if your prediction was `array([0.66764488, 0.33235512])` and actual was `(0,1)`, you would have an error on 0 of `0.66764488` and an error on 1 of `(1  - 0.33235512)`. You would increment the sums for 0 errors and 1 errors accordingly.\n",
    "  2. Now set 2 variables to the average error being made on 0 and 1. I got the following values:<br>\n",
    "  ('average 0 error', 0.48547382904526587)<br>\n",
    "('average 1 error', 0.4875571623785993)\n",
    "  3. Find the predictions that are above average error for *both* 0 and 1. Store the indices of such predictions in bad_samples. Looking back to step 1, would a prediction of `array([0.66764488, 0.33235512])` and actual `(0,1)` be added to bad_samples? Yes. Both are in error beyond the average.\n",
    "  \n",
    "<p>\n",
    "  You can see my final results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "XGgq6gTgaSn3",
    "outputId": "b6dda205-afcb-45a4-a73d-28dba91f2b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 6, 7, 8, 10, 11, 12]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "print(len(bad_samples))\n",
    "bad_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNg9bZtebV2y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practice_final_inclass_w19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
