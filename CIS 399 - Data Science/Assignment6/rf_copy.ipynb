{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zyEMPhCnHC_"
   },
   "source": [
    "<h1>\n",
    "<center>\n",
    "Module 6: Random Forests\n",
    "</center>\n",
    "</h1>\n",
    "<div class=h1_cell>\n",
    "\n",
    "You will be working with the loan table again.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "KJT0kc6FhooT",
    "outputId": "c59d0c37-4342-4702-c467-b6b8755975e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRgA76lahpmR"
   },
   "outputs": [],
   "source": [
    "with open('/content/gdrive/My Drive/class_tables/loan_table_week4.csv', 'r') as f:\n",
    "  loan_table = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RjDDjSE-htzN",
    "outputId": "ccdba93a-46d7-4215-f531-751e1a85f2a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'library_w19_week6.py': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm library_w19_week6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "SCblsgv0nHDB",
    "outputId": "c74bb261-ab89-4c58-82cd-e440ec04beda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-3c39ab47-f270-4292-bd23-d7c4af1ed34f\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-3c39ab47-f270-4292-bd23-d7c4af1ed34f\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving library_w19_week6.py to library_w19_week6.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'library_w19_week6.py': b'import pandas as pd\\nimport numpy as np\\nfrom functools import reduce\\n\\ndef predictor_case(row, pred, target):\\n\\tcase_dict = {(0,0): \\'true_negative\\', (1,1): \\'true_positive\\', (0,1): \\'false_negative\\', (1,0): \\'false_positive\\'}\\n\\tactual = row[target]\\n\\tprediction = row[pred]\\n\\tcase = case_dict[(prediction, actual)]\\n\\treturn case\\n\\ndef accuracy(cases):\\n    tp = cases[\\'true_positive\\'] if \\'true_positive\\' in cases else 0\\n    tn = cases[\\'true_negative\\'] if \\'true_negative\\' in cases else 0\\n    fp = cases[\\'false_positive\\'] if \\'false_positive\\' in cases else 0\\n    fn = cases[\\'false_negative\\'] if \\'false_negative\\' in cases else 0\\n\\n    result = (tp + tn)/(tp+tn+fp+fn) if (tp+tn+fp+fn) != 0 else 0\\n    return result\\n\\ndef f1(cases):\\n    #the heart of the matrix\\n    tp = cases[\\'true_positive\\'] if \\'true_positive\\' in cases else 0\\n    tn = cases[\\'true_negative\\'] if \\'true_negative\\' in cases else 0\\n    fp = cases[\\'false_positive\\'] if \\'false_positive\\' in cases else 0\\n    fn = cases[\\'false_negative\\'] if \\'false_negative\\' in cases else 0\\n    \\n    #other measures we can derive\\n    recall = tp/(tp+fn)  if (tp+fn) != 0 else 0 # positive correct divided by total positive in the table\\n    precision = tp/(tp+fp) if (tp+fp) != 0 else 0 # positive correct divided by all positive predictions made\\n    \\n    #now for the one we want\\n    recall_div = 1/recall if recall != 0 else 0\\n    precision_div = 1/precision if precision != 0 else 0\\n    f1 = 2/(recall_div + precision_div) if (recall_div + precision_div) != 0 else 0\\n    \\n    return f1\\n\\ndef informedness(cases):\\n    tp = cases[\\'true_positive\\'] if \\'true_positive\\' in cases else 0\\n    tn = cases[\\'true_negative\\'] if \\'true_negative\\' in cases else 0\\n    fp = cases[\\'false_positive\\'] if \\'false_positive\\' in cases else 0\\n    fn = cases[\\'false_negative\\'] if \\'false_negative\\' in cases else 0\\n\\n    recall = tp/(tp+fn)  if (tp+fn) != 0 else 0 # positive correct divided by total positive in the table\\n    specificty = tn/(tn+fp) if (tn+fp) != 0 else 0# negative correct divided by total negative in the table\\n    J = (recall + specificty) - 1\\n    return J\\n\\n#starting week 3\\n\\ndef probabilities(counts):\\n    count_0 = 0 if 0 not in counts else counts[0]  #could have no 0 values\\n    count_1 = 0 if 1 not in counts else counts[1]\\n    total = count_0 + count_1\\n    probs = (0,0) if total == 0 else (count_0/total, count_1/total)  #build 2-tuple\\n    return probs\\n\\ndef gini(counts):\\n    (p0,p1) = probabilities(counts)\\n    sum_probs = p0**2 + p1**2\\n    gini = 1 - sum_probs\\n    return gini\\n\\ndef gig(starting_table, split_column, target_column):\\n    \\n    #split into two branches, i.e., two sub-tables\\n    true_table = starting_table.loc[starting_table[split_column] == 1]\\n    false_table = starting_table.loc[starting_table[split_column] == 0]\\n    \\n    #Now see how the target column is divided up in each sub-table (and the starting table)\\n    true_counts = true_table[target_column].value_counts()  # Note using true_table and not starting_table\\n    false_counts = false_table[target_column].value_counts()  # Note using false_table and not starting_table\\n    starting_counts = starting_table[target_column].value_counts() \\n    \\n    #compute the gini impurity for the 3 tables\\n    starting_gini = gini(starting_counts)\\n    true_gini = gini(true_counts)\\n    false_gini = gini(false_counts)\\n\\n    #compute the weights\\n    starting_size = len(starting_table.index)\\n    true_weight = 0.0 if starting_size == 0 else len(true_table.index)/starting_size\\n    false_weight = 0.0 if starting_size == 0 else len(false_table.index)/starting_size\\n    \\n    #wrap it up and put on a bow\\n    gig = starting_gini - (true_weight * true_gini + false_weight * false_gini)\\n    \\n    return gig\\n\\n\\'\\'\\'\\ndef entropy(counts):\\n    (p0, p1) = probabilities(counts)\\n    term1 = -p0*math.log(p0,2) if p0 > 0 else 0\\n    term2 = -p1*math.log(p1,2) if p1 > 0 else 0\\n    entropy = term1 + term2\\n    return entropy\\n\\ndef IGain(starting_table, split_column, target_column):\\n    \\n    #split into two branches, i.e., two sub-tables\\n    true_table = starting_table.loc[starting_table[split_column] == 1]\\n    false_table = starting_table.loc[starting_table[split_column] == 0]\\n    \\n    #Now see how the target column is divided up in each sub-table (and the starting table)\\n    true_counts = true_table[target_column].value_counts()\\n    false_counts = false_table[target_column].value_counts()  # Note using true_table and not titanic_table\\n    starting_counts = starting_table[target_column].value_counts() \\n    \\n    #compute the entropy for the 3 tables\\n    starting_e = entropy(starting_counts)\\n    true_e = entropy(true_counts)\\n    false_e = entropy(false_counts)\\n\\n    #compute the weights\\n    starting_size = len(starting_table.index)\\n    true_weight = 0.0 if starting_size == 0 else 1.0*len(true_table.index)/starting_size\\n    false_weight = 0.0 if starting_size == 0 else 1.0*len(false_table.index)/starting_size\\n    \\n    #wrap it up and put on a bow\\n    infogain = starting_e - (true_weight * true_e + false_weight * false_e)\\n    \\n    return infogain\\n\\'\\'\\'\\n\\n#week 4\\n\\ndef build_pred(column, branch):\\n    return lambda row: row[column] == branch\\n\\ndef find_best_splitter(table, choice_list, target):\\n  \\n    assert (len(table)>0),\"Cannot split empty table\"\\n    assert (target in table),\"Target must be column in table\"\\n    \\n    gig_scores = map(lambda col: (col, gig(table, col, target)), choice_list)  #compute tuple (col, gig) for each column\\n    gig_sorted = sorted(gig_scores, key=lambda item: item[1], reverse=True)  # sort on gig\\n    return gig_sorted\\n\\nfrom functools import reduce\\n\\ndef generate_table(table, conjunction):\\n  \\n    assert (len(table)>0),\"Cannot generate from empty table\"\\n\\n    sub_table = reduce(lambda subtable, pair: subtable.loc[pair[1]], conjunction, table)\\n    return sub_table\\n\\ndef compute_prediction(table, target):\\n  \\n    assert (len(table)>0),\"Cannot predict from empty table\"\\n    assert (target in table),\"Target must be column in table\"\\n    \\n    counts = table[target].value_counts()  # counts looks like {0: v1, 1: v2}\\n\\n    if 0 not in counts:\\n        prediction = 1\\n    elif 1 not in counts:\\n        prediction = 0\\n    elif counts[1] > counts[0]:  # ties go to 0 (negative)\\n        prediction = 1\\n    else:\\n        prediction = 0\\n\\n    return prediction\\n\\ndef build_tree_iter(table, choices, target, hypers={} ):\\n\\n    assert (len(choices)>0),\"Must have at least one column in choices\"\\n    assert (target in table), \"Target column not in table\"\\n    assert (len(table) > 1), \"Table must have more than 1 row\"\\n\\n    hyper_keys = [*hypers]  #fails on 2.7\\n    target_set = set([\\'max-depth\\',\\'gig-cutoff\\'])\\n    diff_set = set(hyper_keys) - target_set\\n    if diff_set: print(\\'WARNING: unrecognized hyper parameters \\' + str(diff_set))\\n    \\n    k = hypers[\\'max-depth\\'] if \\'max-depth\\' in hypers else min(4, len(choices))\\n    gig_cutoff = hypers[\\'gig-cutoff\\'] if \\'gig-cutoff\\' in hypers else 0.0\\n    \\n    def iterative_build(k):\\n        columns_sorted = find_best_splitter(table, choices, target)\\n        (best_column, gig_value) = columns_sorted[0]\\n        \\n        #Note I add _1 or _0 to make it more readable for debugging\\n        current_paths = [{\\'conjunction\\': [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value},\\n                         {\\'conjunction\\': [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value}\\n                        ]\\n        k -= 1  # we just built a level as seed so subtract 1 from k\\n        tree_paths = []  # add completed paths here\\n        \\n        while k>0:\\n            new_paths = []\\n            for path in current_paths:\\n                old_conjunction = path[\\'conjunction\\']  # a list of (name, lambda)\\n                before_table = generate_table(table, old_conjunction)  #the subtable the current conjunct leads to\\n                columns_sorted = find_best_splitter(before_table, choices, target)\\n                (best_column, gig_value) = columns_sorted[0]\\n                if gig_value > gig_cutoff:\\n                    new_path_1 = {\\'conjunction\\': old_conjunction + [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value}\\n                    new_paths.append( new_path_1 ) #true\\n                    new_path_0 = {\\'conjunction\\': old_conjunction + [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value}\\n                    new_paths.append( new_path_0 ) #false\\n                else:\\n                    #not worth splitting so complete the path with a prediction\\n                    path[\\'prediction\\'] = compute_prediction(before_table, target)\\n                    tree_paths.append(path)\\n            #end for loop\\n            \\n            current_paths = new_paths\\n            if current_paths != []:\\n                k -= 1\\n            else:\\n                break  # nothing left to extend so have copied all paths to tree_paths\\n        #end while loop\\n\\n        #Generate predictions for all paths that have None\\n        for path in current_paths:\\n            conjunction = path[\\'conjunction\\']\\n            before_table = generate_table(table, conjunction)\\n            path[\\'prediction\\'] = compute_prediction(before_table, target)\\n            tree_paths.append(path)\\n        return tree_paths\\n\\n    return {\\'paths\\': iterative_build(k), \\'weight\\': None}\\n\\ndef tree_predictor(row, tree):\\n  \\n    assert (len(tree[\\'paths\\']) > 0), \"Tree must have at least one path\"\\n    \\n    #go through each path, one by one (could use a map instead of for loop?)\\n    for path in tree[\\'paths\\']:\\n        conjuncts = path[\\'conjunction\\']\\n        result = map(lambda tuple: tuple[1](row), conjuncts)  # potential to be parallelized\\n        if all(result):\\n            return path[\\'prediction\\']\\n    raise LookupError(\\'No true paths found for row: \\' + str(row))\\n\\n#============== Week 4 version b\\n\\ndef path_id(row, tree):\\n  \\n    assert (len(tree[\\'paths\\']) > 0), \"Tree must have at least one path\"\\n    \\n    #go through each path, one by one (could use a map instead of for loop?)\\n    for i, path in enumerate( tree[\\'paths\\']):\\n        conjuncts = path[\\'conjunction\\']\\n        result = map(lambda tup: tup[1](row), conjuncts)  # potential to be parallelized\\n        if all(result):\\n            return i\\n    raise LookupError(\\'No true paths found for row: \\' + str(row))\\n\\ndef reorder_paths(table, tree):\\n  \\n    new_paths = []\\n    paths = tree[\\'paths\\']\\n\\n    tempcounts = table.apply(lambda row: path_id(row, tree), axis=1)\\n    valcounts= tempcounts.value_counts()\\n    dictcounts = dict(valcounts)\\n    listcounts = list(dictcounts.items())\\n    sortedcounts = sorted(listcounts, key=lambda x: x[1], reverse=True)\\n    print(sortedcounts)\\n    \\n    for tup in sortedcounts:\\n        new_paths.append(paths[tup[0]])\\n        \\n    return new_paths\\n\\n#============== Week 5\\nfrom types import SimpleNamespace\\n\\ndef produce_scores(table, tree, target):\\n    scratch_table = pd.DataFrame(columns=[\\'prediction\\', \\'actual\\'])\\n    scratch_table[\\'prediction\\'] = table.apply(lambda row: tree_predictor(row, tree), axis=1)\\n    scratch_table[\\'actual\\'] = table[target]  # just copy the target column\\n    cases = scratch_table.apply(lambda row: predictor_case(row, pred=\\'prediction\\', target=\\'actual\\'), axis=1)\\n    vc = cases.value_counts()\\n    return [accuracy(vc), f1(vc), informedness(vc)]\\n\\ndef compute_training(slices, left_out):\\n    training_slices = []\\n    for i,slice in enumerate(slices):\\n        if i == left_out:\\n            continue\\n        training_slices.append(slices[i])\\n    return pd.concat(training_slices)  # note we are returning a table (DataFrame)\\n\\ndef k_fold(table, k, target, hypers, candidate_columns):\\n\\n    result_columns = [\\'name\\',  \\'accuracy\\', \\'f1\\', \\'informedness\\']\\n    k_fold_results_table = pd.DataFrame(columns=result_columns)\\n    \\n    total_len = len(table.index)\\n    slice_size = int(total_len/(1.0*k))\\n\\n    #generate the slices\\n    slices = []\\n    for i in range(k-1):\\n        a_slice =  table[i*slice_size:(i+1)*slice_size]\\n        slices.append( a_slice )\\n    slices.append( table[(k-1)*slice_size:] )  # whatever is left\\n    \\n    #do cross validation\\n    all_scores = []\\n    for i in range(k):\\n        test_table = slices[i]\\n        train_table = compute_training(slices, i)\\n        fold_tree = build_tree_iter(train_table, candidate_columns, target, hypers)  # train\\n        scores = produce_scores(test_table, fold_tree, target)  # test\\n        result_row = {\\'name\\':\\'fold_\\'+str(i), \\'accuracy\\':scores[0], \\'f1\\':scores[1], \\'informedness\\':scores[2]}\\n        k_fold_results_table = k_fold_results_table.append(result_row,ignore_index=True)\\n        all_scores.append(scores)\\n        \\n    #compute the average\\n    average = list(reduce(np.add, all_scores[1:], all_scores[0])/k)\\n    result_row = {\\'name\\':\\'average\\', \\'accuracy\\':average[0], \\'f1\\':average[1], \\'informedness\\':average[2]}\\n    k_fold_results_table = k_fold_results_table.append(result_row,ignore_index=True)\\n    \\n    return k_fold_results_table\\n\\n########  Added from assignment notebook\\n\\n\\n\\ndef k_fold_random(table, k, target, hypers, candidate_columns):\\n\\n    result_columns = [\\'name\\',  \\'accuracy\\', \\'f1\\', \\'informedness\\']\\n    k_fold_results_table = pd.DataFrame(columns=result_columns)\\n    \\n    total_len = len(table.index)\\n    slice_size = int(total_len/(1.0*k))\\n\\n    #generate the slices\\n    \\n    #your new code here\\n    sub_table = table\\n    slices = []\\n    for i in range(k-1):\\n        new_slice = sub_table.sample(n=slice_size)\\n        sub_table = sub_table.loc[~sub_table.index.isin(new_slice.index)]\\n        slices.append( new_slice )\\n    slices.append( sub_table )\\n    verify_unique(slices)\\n    \\n    #do cross validation\\n    all_scores = []\\n    for i in range(k):\\n        test_table = slices[i]\\n        train_table = compute_training(slices, i)\\n        fold_tree = build_tree_iter(train_table, candidate_columns, target, hypers)  # train\\n        scores = produce_scores(test_table, fold_tree, target)  # test\\n        result_row = {\\'name\\':\\'fold_\\'+str(i), \\'accuracy\\':scores[0], \\'f1\\':scores[1], \\'informedness\\':scores[2]}\\n        k_fold_results_table = k_fold_results_table.append(result_row,ignore_index=True)\\n        all_scores.append(scores)\\n        \\n    #compute the average\\n    average = list(reduce(np.add, all_scores[1:], all_scores[0])/k)\\n    result_row = {\\'name\\':\\'average\\', \\'accuracy\\':average[0], \\'f1\\':average[1], \\'informedness\\':average[2]}\\n    k_fold_results_table = k_fold_results_table.append(result_row,ignore_index=True)\\n    \\n    return k_fold_results_table\\n\\n#Determine if slices are mutually exclusive\\ndef verify_unique(slices):\\n    print((\\'total length all slices\\', sum([len(s) for s in slices])))\\n    for i, a_slice in enumerate(slices[:-1]):\\n        a_set = set(a_slice.index)\\n        for j, b_slice in enumerate(slices[i+1:]):\\n            b_set = set(b_slice.index)\\n            int_set = a_set.intersection(b_set)  # should be empty set as result\\n            print((i,j+i+1,int_set))\\n    return None\\n\\n####  week 6\\n\\nimport random\\n\\ndef vote_taker(row, forest):\\n    votes = {0:0, 1:0}\\n    for tree in forest:\\n        prediction = tree_predictor(row, tree)\\n        votes[prediction] += 1\\n    winner = 1 if votes[1]>votes[0] else 0  #ties go to 0\\n    return winner\\n\\n\\ndef forest_scores(table, forest, target):\\n    scratch_table = pd.DataFrame(columns=[\\'prediction\\', \\'actual\\'])\\n    scratch_table[\\'prediction\\'] = table.apply(lambda row: vote_taker(row, forest), axis=1)  #only change is to call vote_taker\\n    scratch_table[\\'actual\\'] = table[target]  # just copy the target column\\n    cases = scratch_table.apply(lambda row: predictor_case(row, pred=\\'prediction\\', target=\\'actual\\'), axis=1)\\n    vc = cases.value_counts()\\n    return [accuracy(vc), f1(vc), informedness(vc)]\\n\\n\\ndef forest_builder(table, column_choices, target, hypers):\\n    \\n    tree_n = 5 if \\'total-trees\\' not in hypers else hypers[\\'total-trees\\']\\n    m = int(len(column_choices)**.5) if \\'m\\' not in hypers else hypers[\\'m\\']\\n    k = hypers[\\'max-depth\\'] if \\'max-depth\\' in hypers else min(2, len(column_choices))\\n    gig_cutoff = hypers[\\'gig-cutoff\\'] if \\'gig-cutoff\\' in hypers else 0.0\\n    rgen = hypers[\\'random-state\\'] if \\'random-state\\' in hypers else 0  #an int will work as seed with the sample method.\\n\\n    #build a single tree of depth n - call it multiple times to build multiple trees\\n    def iterative_build(n):\\n        train = table.sample(frac=1.0, replace=True, random_state=rgen)\\n        train = train.reset_index()\\n        left_out = table.loc[~table.index.isin(train[\\'index\\'])]\\n        left_out = left_out.reset_index() # this gives us the old index in its own column\\n        oob_list = left_out[\\'index\\'].tolist()  # list of row indices from original titanic table\\n        \\n        rcols = random.sample(column_choices, m)  # subspcace sampling - uses random.seed, not rng\\n        columns_sorted = find_best_splitter(train, rcols, target)\\n        (best_column, gig_value) = columns_sorted[0]\\n\\n        #Note I add _1 or _0 to make it more readable for debugging\\n        current_paths = [{\\'conjunction\\': [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value},\\n                         {\\'conjunction\\': [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                          \\'prediction\\': None,\\n                          \\'gig_score\\': gig_value}\\n                        ]\\n        n -= 1  # we just built a level as seed so subtract 1 from n\\n        tree_paths = []  # add completed paths here\\n\\n        while n>0:\\n            new_paths = []\\n            for path in current_paths:\\n                conjunct = path[\\'conjunction\\']  # a list of (name, lambda)\\n                before_table = generate_table(train, conjunct)  #the subtable the current conjunct leads to\\n                rcols = random.sample(column_choices, m)  # subspace\\n                columns_sorted = find_best_splitter(before_table, rcols, target)\\n                (best_column, gig_value) = columns_sorted[0]\\n                if gig_value > gig_cutoff:\\n                    new_path_1 = {\\'conjunction\\': conjunct + [(best_column+\\'_1\\', build_pred(best_column, 1))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value}\\n                    new_paths.append( new_path_1 ) #true\\n                    new_path_0 = {\\'conjunction\\': conjunct + [(best_column+\\'_0\\', build_pred(best_column, 0))],\\n                                \\'prediction\\': None,\\n                                 \\'gig_score\\': gig_value\\n                                 }\\n                    new_paths.append( new_path_0 ) #false\\n                else:\\n                    #not worth splitting so complete the path with a prediction\\n                    path[\\'prediction\\'] = compute_prediction(before_table, target)\\n                    tree_paths.append(path)\\n            #end for loop\\n\\n            current_paths = new_paths\\n            if current_paths != []:\\n                n -= 1\\n            else:\\n                break  # nothing left to extend so have copied all paths to tree_paths\\n        #end while loop\\n\\n        #Generate predictions for all paths that have None\\n        for path in current_paths:\\n            conjunct = path[\\'conjunction\\']\\n            before_table = generate_table(train, conjunct)\\n            path[\\'prediction\\'] = compute_prediction(before_table, target)\\n            tree_paths.append(path)\\n        return (tree_paths, oob_list)\\n    \\n    #let\\'s build a forest\\n    forest = []\\n    for i in range(tree_n):\\n        (paths, oob) = iterative_build(k)  #always use k for now\\n        forest.append({\\'paths\\': paths, \\'weight\\': None, \\'oob\\': oob})\\n        \\n    return forest\\n\\n'}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "rtmSW_2BnHDF",
    "outputId": "322f66a5-0dbe-45c7-ef27-490a7ce05c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\t build_pred\t build_tree_iter\t compute_prediction\t compute_training\t f1\t find_best_splitter\t forest_builder\t forest_scores\t \n",
      "generate_table\t gig\t gini\t informedness\t k_fold\t k_fold_random\t path_id\t predictor_case\t probabilities\t \n",
      "produce_scores\t reorder_paths\t tree_predictor\t verify_unique\t vote_taker\t \n"
     ]
    }
   ],
   "source": [
    "from library_w19_week6 import *\n",
    "\n",
    "%who function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "YnHkUajjnHDQ",
    "outputId": "8258d525-1377-4e27-eace-983561a18c50",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>no_lam</th>\n",
       "      <th>filled_lam</th>\n",
       "      <th>pa_Rural</th>\n",
       "      <th>pa_Semiurban</th>\n",
       "      <th>pa_Urban</th>\n",
       "      <th>pa_nan</th>\n",
       "      <th>lam_bin</th>\n",
       "      <th>lam_Low</th>\n",
       "      <th>lam_Average</th>\n",
       "      <th>lam_High</th>\n",
       "      <th>ch_bad</th>\n",
       "      <th>ch_good</th>\n",
       "      <th>ch_nan</th>\n",
       "      <th>apin_binned</th>\n",
       "      <th>apin_low</th>\n",
       "      <th>apin_average</th>\n",
       "      <th>apin_high</th>\n",
       "      <th>apin_nan</th>\n",
       "      <th>dep_0</th>\n",
       "      <th>dep_1</th>\n",
       "      <th>dep_2</th>\n",
       "      <th>dep_3+</th>\n",
       "      <th>dep_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146.412162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Married Dependents     Education Self_Employed  ApplicantIncome  \\\n",
       "0   Male      No          0      Graduate            No             5849   \n",
       "1   Male     Yes          1      Graduate            No             4583   \n",
       "2   Male     Yes          0      Graduate           Yes             3000   \n",
       "3   Male     Yes          0  Not Graduate            No             2583   \n",
       "4   Male      No          0      Graduate            No             6000   \n",
       "\n",
       "   CoapplicantIncome  LoanAmount  Loan_Amount_Term  Credit_History  \\\n",
       "0                0.0         NaN             360.0             1.0   \n",
       "1             1508.0       128.0             360.0             1.0   \n",
       "2                0.0        66.0             360.0             1.0   \n",
       "3             2358.0       120.0             360.0             1.0   \n",
       "4                0.0       141.0             360.0             1.0   \n",
       "\n",
       "  Property_Area  Loan_Status  no_lam  filled_lam  pa_Rural  pa_Semiurban  \\\n",
       "0         Urban            1       1  146.412162         0             0   \n",
       "1         Rural            0       0  128.000000         1             0   \n",
       "2         Urban            1       0   66.000000         0             0   \n",
       "3         Urban            1       0  120.000000         0             0   \n",
       "4         Urban            1       0  141.000000         0             0   \n",
       "\n",
       "   pa_Urban  pa_nan lam_bin  lam_Low  lam_Average  lam_High  ch_bad  ch_good  \\\n",
       "0         1       0     Low        1            0         0       0        1   \n",
       "1         0       0     Low        1            0         0       0        1   \n",
       "2         1       0     Low        1            0         0       0        1   \n",
       "3         1       0     Low        1            0         0       0        1   \n",
       "4         1       0     Low        1            0         0       0        1   \n",
       "\n",
       "   ch_nan apin_binned  apin_low  apin_average  apin_high  apin_nan  dep_0  \\\n",
       "0       0         low         1             0          0         0      1   \n",
       "1       0         low         1             0          0         0      0   \n",
       "2       0         low         1             0          0         0      1   \n",
       "3       0         low         1             0          0         0      1   \n",
       "4       0         low         1             0          0         0      1   \n",
       "\n",
       "   dep_1  dep_2  dep_3+  dep_nan  \n",
       "0      0      0       0        0  \n",
       "1      1      0       0        0  \n",
       "2      0      0       0        0  \n",
       "3      0      0       0        0  \n",
       "4      0      0       0        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "loan_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "gVKmmwDqnHDW",
    "outputId": "3f82e95a-49e7-43df-81c1-a5f7d5072900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
       "       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
       "       'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n",
       "       'Loan_Status', 'no_lam', 'filled_lam', 'pa_Rural', 'pa_Semiurban',\n",
       "       'pa_Urban', 'pa_nan', 'lam_bin', 'lam_Low', 'lam_Average',\n",
       "       'lam_High', 'ch_bad', 'ch_good', 'ch_nan', 'apin_binned',\n",
       "       'apin_low', 'apin_average', 'apin_high', 'apin_nan', 'dep_0',\n",
       "       'dep_1', 'dep_2', 'dep_3+', 'dep_nan'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_table.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpXRjQHtnHDa"
   },
   "source": [
    "<hr>\n",
    "<h1>\n",
    "1: Explore forest options (20)\n",
    "</h1>\n",
    "<p>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Check out the results you get from forests of size 5, 11, 17.\n",
    "<p>\n",
    "First, define the columns to use. I do that for you below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNRyB8DvnHDc"
   },
   "outputs": [],
   "source": [
    "splitter_columns = [\n",
    "        #Dependents\n",
    "        'dep_0', 'dep_1', 'dep_2', 'dep_3+',\n",
    "        #ApplicantIncome\n",
    "       'apin_low', 'apin_high', 'apin_average',\n",
    "        #Property_Area\n",
    "        'pa_Rural', 'pa_Semiurban','pa_Urban',\n",
    "        #LoanAmount\n",
    "        'lam_Low', 'lam_Average', 'lam_High',\n",
    "        #Credit_History\n",
    "        'ch_bad', 'ch_good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPLu2ub9nHDg"
   },
   "source": [
    "<hr>\n",
    "<h2>\n",
    "Set seeds so get consistent results\n",
    "</h2>\n",
    "<p>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1eNjUiSnW0p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "rng = np.random.RandomState(42)  #Will pass as arg to pandas sample method\n",
    "random.seed(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iuhvnqw3nHDh",
    "outputId": "90d531f5-5e59-4969-d05b-e4649fb1de6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest1 = \n",
    "len(forest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-fCS27OonHDm",
    "outputId": "10e9f8a7-a225-4ba0-d762-ceaddb00079c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8094462540716613, 0.8764519535374868, 0.4104956556082149]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_scores(loan_table, forest1, 'Loan_Status')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rjc3x2M8nHDr",
    "outputId": "570db65f-05d9-4071-ac39-b22ac0a6543e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest2 = \n",
    "len(forest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IXRcCWcpnHDx",
    "outputId": "4d18cf65-16e0-432c-b975-cc077c46ada9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7785016286644951, 0.8597938144329897, 0.3058599921011058]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_scores(loan_table, forest2, 'Loan_Status')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I-n4Tf5bnHD2",
    "outputId": "a2a38dcf-3226-4942-b62e-8556adca5241"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest3 = \n",
    "len(forest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sh3evOrbnHD8",
    "outputId": "4ce04a00-993b-4896-a7de-03f2cbca3bc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8094462540716613, 0.8764519535374868, 0.4104956556082149]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_scores(loan_table, forest3, 'Loan_Status')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfoOXy5JnHEA"
   },
   "source": [
    "<hr>\n",
    "<h1>\n",
    "2: Implement Out of Bag testing (80)\n",
    "</h1>\n",
    "<p>\n",
    "<div class=h1_cell>\n",
    "<p>\n",
    "Last module we looked at the use of K-Folding as a means to test our trees. Random Forests give us an alternative by using out of bag testing. Using notes from the content notebook this week, find a way to do prediction using the oob idea. As reminder, the set union of all the oob lists in a forest make up the testing set. If there is a row in loan_table that is not in any oob list, that row should be omitted from the test set. Further, a tree only gets to vote on a specific row if that row is in the tree's oob list.\n",
    "  <p>\n",
    "  I am going to leave it to you to come up with an algorithm for doing oob testing. If you get totally stuck, I can supply hints. For grading I am looking to make sure you only use oob rows for testing and that each individual tree only votes on rows in its own oob list.\n",
    "    <p>\n",
    "      It is worthwhile solving this problem given something like it will likely be on next midterm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYlMHr1RnHEK"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9gDrRf8p1E1"
   },
   "source": [
    "<h2>Check your oob testing against my results</h2>\n",
    "\n",
    "If you used the random seeds to build your trees, your results should be the same as mine. No randomness during oob testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5PW22KtotRM"
   },
   "outputs": [],
   "source": [
    "#whole table from above: [0.8094462540716613, 0.8764519535374868, 0.4104956556082149]\n",
    "\n",
    "oob_forest_scores(testing_table_1, forest1, 'Loan_Status')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeopvaI5psYx"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wcj4Oy5vpxlW"
   },
   "outputs": [],
   "source": [
    "#from above: [0.7785016286644951, 0.8597938144329897, 0.3058599921011058]\n",
    "\n",
    "oob_forest_scores(testing_table_2, forest2, 'Loan_Status')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ut4tfLw0p-k2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_bnlpv-p_mm"
   },
   "outputs": [],
   "source": [
    "#from above: [0.8094462540716613, 0.8764519535374868, 0.4104956556082149]\n",
    "\n",
    "oob_forest_scores(testing_table_3, forest3, 'Loan_Status')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ofRzrrInfGp"
   },
   "source": [
    "<h2>Not a lot of change</h2>\n",
    "\n",
    "Using oob testing did not affect scores much. I think we would need to work with bigger tables, e.g., the 25K shelter table, to see a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqg9TP5Cqpy8"
   },
   "source": [
    "<hr>\n",
    "<h1>Write it out</h1>\n",
    "<div class=h1_cell>\n",
    "\n",
    "Did not change table but we did define new functions. Add them to your library as `!rm library_w19_week6b.py`. I added the `b` to designate functions from assignment portion of module.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rf_assignment_w19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
